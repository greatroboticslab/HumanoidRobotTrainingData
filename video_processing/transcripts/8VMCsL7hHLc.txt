Challenges finding information in food and agriculture on the Web: what can we do better?
https://www.youtube.com/watch?v=8VMCsL7hHLc
Science & Technology
 Good morning and also good afternoon for those that are joining from Europe. I'm very pleased to chair this panel today at the online DCMI 2020 conference dedicated to challenges finding information in food and agriculture on the web. What can we do better? And this panel is going to focus on food and agricultural sciences. And as you will learn over the next 90 minutes, agriculture and disciplines related have special characteristics that make data interoperability particularly challenging. And so the work of search engines to collect scientific data. From one side, in this particular sciences, grey literature is crucial. While journal articles are not necessarily the only scholarly communication channel that counts. And secondly, the most important and that is going to be very much highlighted today, is that while in other sciences, English is the pivotal language. In the case of the food and agriculture and due to the diversity of languages behind used, it's necessary to consider multilinguism and semantic strategies as a way to increase accessibility to all the scientific literature produced. We hope that at the end of this session, panelists will share some thoughts about possible ways to address some of the identified challenges, as well possible common synergies to build on how to bring them forward. Today, we have panelists from the Food and Agriculture Organization of United Nations, CGIR, Land Portal Foundation, USDA National Agricultural Library, and the Beijing Academy of Agriculture and Forestry Sciences, that unfortunately is not going to be physically in this panel session due to time constraints, the time zone constraints, but we have a recording to present the use case that they would like to share with you about the work that they are doing on this field. We encourage all of you to submit your questions through the chat box any time. Don't be shy, just share. If you have any specific request for one panelist, please add her name at the beginning of your comment. This will make it easy to go through the questions and answers part. And don't forget that we would like to hear from you as well. This is an interactive session where we would like also to learn from your thoughts and experiences, if you have any in this field or in any other field that you think that could be also of interest for the audience. So this panel will start with a short presentation by each of the panelists, and this is going to bring us to take more or less 60 minutes. And after that, we will start with the questions and answers. But before we continue with the session, let me introduce myself, because of course, probably you don't know me at all. So I'm Ima Subiraz. I'm a senior information management at FAO, and I'm the program manager for AgDIS and AgDavoc, which will be also presented today during our discussion. Said that, you have here in the Zoom all the panelists. We will start by the order that was displayed in the announcement of this session. Before I would like to introduce our first panelist, which is Erin Antonioli. Erin is coming from the USDA National Agricultural Library, NAL, and she is the lead metadata librarian and data creation team in the knowledge services division of the USDA NAL. Thank you, Erin. And you can share your presentation. It's over to you. Thank you, Ima. Can you hear me? Perfectly. Yep. Thank you. Excellent. Excellent. Thank you. I'm here today to talk about the AgData Commons, USDA's general AgData catalog and repository, which is designed to help USDA researchers publish and disseminate data products. The National Agricultural Library is one of five national libraries of the United States. It houses one of the world's largest collections devoted to agriculture and its related sciences. NAL provides a variety of online resources, including journal articles, digital collections, and other materials relating to agriculture. I'm going to focus on the AgData Commons for today's presentation. The AgData Commons is a catalog and data repository for USDA funded research data. It provides expert services for creating, curating, and enabling access to complete and machine readable scientific metadata. It creates infrastructure for linking information, data, publications, people, and more. And it helps the USDA funded research community meet public access requirements. An AgData Commons dataset consists of data files and associated metadata. We provide tools and services to make each dataset richer and more discoverable. The AgData Commons strives to deliver fair metadata that are machine readable and available via APIs and other automated means. As a generalist Ag repository, we focus most heavily on the F and the A of this matrix, since interoperability and reusability generally require subject specific adherence to more regimented data and metadata standards. To that end, the AgData Commons provides discovery and access as its core function. Search and discovery tools and linked information promote findability. We enable access through the catalog and public APIs, use of open licenses, and DOIs for digital assets. The AgData Commons synthesizes metadata to achieve a variety of services. I like the analogy of Lego blocks to represent AgData Commons metadata because the pieces come in different colors, shapes, and sizes, and can be assembled in a variety of ways to perform a variety of functions. Our metadata system is no different. We ask for essential information about the data and rearrange that input to satisfy the needs of each service that the AgData Commons offers. We often work with geospatial metadata, which must use the ISO 19115 standard. And this standard underpins our internal systems because it's robust enough to handle the information from the other standards. One use case that illustrates this metadata synthesis includes our work with the AgCross repository. AgCross specializes in geospatial data and uses an ISO 19115 flavor of metadata in an Esri platform. We ingest metadata and data by harvesting from AgCross using microservices architecture according to best practices, and create the AgData Commons record and store the data files locally. We then reformat that metadata to the data site format and reserve a DOI using an internal system, which then gets added back to the AgData Commons record. Finally, the DOI is pushed back to AgCross for display with their record using a RESTful API. Records tagged with specific bureau codes are also forwarded to data.gov. This way, AgCross researchers can utilize their subject-specific platform, but also benefit from the discovery and access services the AgData Commons offers. We offer a wide assortment of fields to data submitters with a great deal of linking potential, but we need to strike a balance between information that makes data fair and how much data submitters are willing to fill out. When we present too many requirements, people simply won't fill out, they simply just won't submit their data, or they opt for repositories with a lower bar of entry. When we allow submissions with too few completed fields, the dataset findability and potential reusability suffers. After reviewing the requirements for the services AgData Commons facilitates, we arrived at this core set of essential metadata for each record. This list doesn't account for some of the subject-specific requirements certain research communities may need, but we cover our bases as a generalist Ag catalog with this set of metadata. During the curation process, our staff also adds NAL Thesaurus terms to AgData Commons records. The NAL Thesaurus is used to select controlled vocabulary terms for subject indexing in several of the library's databases. The Thesaurus is produced in part by the National Agricultural Library, is updated annually, and contains over 260,000 terms. It's available as linked open data for anyone to use. The AgData Commons provides the infrastructure for linking metadata and objects to enable analytical tools. We provide specific fields for these relationships to maximize the linking and machine readability of the metadata. This includes different types of relationships between datasets and documentation, author identification links like ORCID, and version guidance. We provide features to promote credit for USDA researchers as an incentive to deposit data with us, and to provide more complete records with linked information. U.S. federal data should be openly and freely available, but the AgData Commons makes it easier for researchers to give and receive credit for their work. Citations are automatically generated for all records in the catalog, and researchers can link to their scholarly publications, data papers, and other related content. We also provide DOIs for locally held data files, which has been a big incentive for researchers to share their data publicly. The AgData Commons provides a lot of information to help us with a variety of different things. Now, as much as we do well, there are still plenty of challenges we face in making data findable and accessible. Our search features offer a particular opportunity for improvement. We have been working with our stakeholders to develop search requirements to use while we evaluate database platforms for our 2021 upgrade. Our data content represents a diverse set of research topics and metadata needs. We can't be all things to all people and instead focus on the services we offer and the metadata required to accomplish those tasks. NAL Thesaurus application is not currently automated, and that's a problem. We plan to work more closely with NAL's indexing and informatics branch to improve NALT term application and provide better linkage between various library systems. And yet another challenge involves which controlled vocabularies to implement and how to link them to authoritative sources. We chose taxonomies related directly to NAL services, but seek a better and more effective way to integrate controlled vocabularies used by the different subject specific areas we support. Our metadata enriches our datasets, but doesn't always make them more discoverable outside of our own platform. Some of this relates to infrastructure and some to culture. For instance, ORCID accounts can automatically link datasets with DOIs we issue from their author profile pages, but many account holders don't turn on that feature. We can capture rich information, but strive to increase the linked information we provide. To that end, we work to convey the value of linked information to the researchers submitting the data. More fields often represents more burden for data submitters. So we look for ways to make the submission process easier and to communicate the added value. As information professionals, we understand how complete and linked information and information are available. So we look for ways to make sure that linked information enhances datasets. However, to get the absolute most utility out of datasets, we must educate submitters and communicate the value of linked information while lessening the submission burden. So data becomes more easily findable and accessible. Thank you. That's my presentation. Here are a few of the links that I showed throughout my presentation and I will hand it back to Ema. Thank you so much, Erin, also for really doing on time. Very interesting presentation. I would like to bring on board our next panelist, Fabrizio Celli. Fabrizio is part of the team that is managing Agris, the International System for Agricultural Science and Technology. Actually, he is the technical leader and he is working for the Food and Agriculture Organization of United Nations. Welcome to the panel, Fabrizio, and over to you. Thank you very much, Emma. Okay. In this session, we will be looking at how Agris is addressing challenges in finding and integrating research in food and agriculture. I want to give you some background about FAO and AGRIS. FAO is the Food and Agriculture Organization of the United Nations and it's the organization that is maintaining AGRIS. FAO's goal is to lead international efforts to defeat hunger and to improve nutrition and food security. It was founded in 1945 and the headquarters is in Rome, even if FAO operates in over 130 countries. In 1974, FAO set up an initiative together with the member countries to make information on agriculture globally available. The outcome of this initiative was AGRIS. AGRIS is a database of almost 12 million of multilingual bibliographic metadata on agricultural research, but it is also a big network of, as you can see, 434 data providers from 150 countries. The importance of AGRIS is also given by the fact that it is highly used. So according to Google Analytics, AGRIS receives around 8 million of visits per month and is accessed from all over the world, 200 countries and territories. Before talking about the new challenges that AGRIS has to address, I want to describe a little bit how the data providers, the AGRIS data providers changed over the time. So at the beginning, in 1974, governments designated official AGRIS centers whose role was to collect all the scientific production in their country and to send the data to AGRIS. Then from 2005 AGRIS started to accept also data from institutional repositories from publishers and from aggregators. Usually in this case, AGRIS doesn't go to an institution and ask for data, but thanks to the visibility that AGRIS has, institutions come to AGRIS asking if they can publish data. And we can face here problems of integration of the data. On the other hand, with the evolution of the technology and also of the open access institutional repositories, AGRIS has to cope also with problems related to automatic discovery, automatic harvesting and processing of data and metadata available on the web. So just to summarize, there are two big areas in terms of challenges. On one side, we have the problem of the integration of new data. So when there is a new data provider, there are several problems to be solved in order to guarantee that the metadata can be correctly published in AGRIS and that can benefit of all the features that give a lot of visibility to AGRIS data. So for example, considering the variety of the data providers and the fact that they come from all over the world, they always use different metadata formats and different standards. And most important, there are very different levels of metadata quality. So while there are data providers who has made very rich metadata records, on the other hand, we can find some data providers with very poor records. And this of course create a problem of balance inside the AGRIS database. Other issues are, for example, related to the full text links and also the stability of the links. AGRIS always suggest to add the link to the full text. But considering that AGRIS started in 1974, most of the links are corrupted. So AGRIS needs to periodically review all the links to guarantee that they are still working. Then the other important problem is about multilingualism. In fact, considering that AGRIS has metadata in up to 90 languages, some problems can exist when AGRIS receives records, especially with text encoding. Everybody should use a Unicode, UTF-8, in order to guarantee interoperability of the metadata in all the languages. The second area is about the automatic discovery and the ingestion of data directly from web APIs. For example, using the OI PMH protocol or using like RESTful APIs. Two big problems here are the relevance of high volume data. In fact, when you search for data on the web and you want to automatically harvest and ingest data into the database, it is difficult to be sure about the relevance. So the fact that the data is relevant to the topics available in the AGRIS database. The second issue is related to content classification and to the integration of this data automatically harvested from the web. Now about the data integration, AGRIS has set up an orchestration of software components. Even if at the beginning we have a manual validation of the metadata. So there is a physical person who receives the metadata and checks the metadata for in order to understand this. First of all, it is relevant to the AGRIS domain. And second, if there are semantics error, for example, or problems with encoding. This is something that is possible for regular data providers, but not when metadata are automatically harvested from the web, because there the volume of the data is very high and it's impossible to do this manually. After this is done, AGRIS has a lot of processes to convert and harmonize all the metadata from a lot of formats to an AGRIS internal format, which is AGRIS AP and then to perform some operations like data cleaning and metadata enrichment in order to reach the final version of the record, which is the AGRIS RDF. So these allows AGRIS to enable the usage of linked open data technologies. So AGRIS is not giving any suggestions in terms of input metadata format. AGRIS tries to accept everything, even if there are several challenges here, poor metadata. So like missing metadata fields, some lack, for example, of controlled vocabularies, wrong usage of metadata properties. And as I told you, in case of multilingual records, also problems with encodings. This is why AGRIS recommends to consider the LODBD recommendations, which are recommendations in terms of metadata properties to use and also in terms of vocabularies to use. And they can help to standardize a little bit the situation and to help the integration of the metadata coming from such a variety of data providers. Of course, it's important to build also some capacity development. This is because AGRIS has learned a lot over the time and the only way to help data providers to produce better metadata is to train also data providers. So the first step is email exchange. So when there are new data providers and AGRIS understands that there are errors or semantic errors or other kinds of problems, there is a chain of email exchange with the data provider to guide the data provider to produce better, richer and meaningful metadata. Then there are also webinars, guides on the websites and recommendations, as I told you before, with the LODBD recommendations. Now, an important piece of the AGRIS infrastructure is AGRAVOC. This is a controlled vocabulary covering all the areas of interest of FAO. It's translated into 39 languages and it's the backbone of AGRIS. So this is used, for example, to help AGRIS with automatic data discovery and classification. AGRAVOC, in fact, is used within some machine learning algorithms that allows AGRIS to understand the relevance of the metadata, which are automatically harvested and downloaded from the web. Plus, AGRAVOC is also very important to enable a lot of additional features on the website, like the multilingual search, like the improvement of the precision and the recall, like the possibility to connect different resources. So using linked open data technologies. And as I told you also in several machine learning algorithms, both to determine the relevance of harvested resources, but also to classify resources according using these multilingual thesaurus. And that's it from my side. Thank you very much. Thank you very much. Fabrizio. So we move now to our third speaker. Shio-Yin King is from the Institute of Agricultural Information and Economics at the Beijing Academy of Agricultural and Forestry Sciences. She's the responsible for the construction of agricultural information resources, data sharing, information analysis and semantic technology research. As I mentioned before, unfortunately, she couldn't attend this panel session. I'm not sure whether all of you can see the video. Yes. Okay. Could you confirm as well that you can hear the sound? Hi. I'm Tia-Ting from China. Okay. So I will put the video now. It's about nine minutes. And then we will move to the next panelist. China. Great honor to have this opportunity to communicate with you. Now I would like to introduce some of the work our team has been here. So I will put the video in the next panelist. China. Great honor to have this opportunity to communicate with you. Now I would like to introduce some of the work our team has been here. For the construction and selling of agricultural information resources, challenges and how to deal with these challenges in the future. First, a brief introduction to my organization. BAFS was founded in 1958. It's a comprehensive agriculture research institution in Beijing. There are 14 specialized research institutes and the business fields cover all aspects of agriculture. There are more than one thousand employees. AIE is a specialized research institute under BAFS. It mainly carries out research and extension surveys in the field of agricultural information resource construction, agricultural information technology, agricultural economic management, etc. Our team has been engaged in the construction of agricultural information resources, data theory and information analysis for nearly two decades. We set up the Beijing Agriculture Think Tank Platform in 2017. The goal of this platform is to build a resources resource, well organized and functional agriculture sense and technology service platform. To provide data theory and service and the in-depth information analysis service for decision makers, managers and researchers. We have been working towards this goal. At present, the platform has gathered more than one million pieces of agricultural information resources from China, other countries and a few international organizations. According to the resource types, the platform brings together 12 categories of information resources including patents, news policies, etc. According to the agriculture theme, the data is divided into seed industry, planting industry, animal husbandry, fishery, etc. At present, the resources is mainly in Chinese and English. Resources formats include text, PDF, numerical values, etc. There are four main channels for data acquisition. One is the internal data of BAFS. BAFS owns a lot of primary data resources such as the expert information, technology achievements, research reports. Another one is the data from the government and the academic association. We get it through comprehensive construction, data exchange and so on. Secondly, the open data from the internet. The last one is the commercial databases. I think a good resource-searing platform should have several characteristics. Abundant resources. Abundant resources. Authoritative and reliable. Timely in updated. Standardized. Good in continuity. For data, special picture and data organization. metadata and semantic are key. Mitigate can change the chaotic data into standardized and available data. Facilitated data management and data quality assessment. Facilitated data integrate and salary. Semantic technology can deploy, digest, and many potential relationships among data. And can be applied, can be applied to resource retrieval, resource recommendation, data association, data discovery, etc. We have also done some preliminary work in the regard. Mitigate standards has been established to standardize the data processing and management. A core metadata model of agriculture information resource dataset is established by fully considering the metadata content necessary for data content, data display, data management, etc. including include the title, subject, abstract, keyword, type, provider, etc. On the basis of common metadata, different data bases extend and aid their own unique metadata fields so as to form the metadata standards of this database. Such as science and technology project, increase the project and commitment unit, funding amount and project time, etc. In the end, we direct the 12 types of metadata standards. When resources are collected on the platform, they are converted into standardized data and stored in the database through format conversation, data cleaning, data attribute extraction, data extension, and other procedures. STCourse is adopted as the base vocabulary of knowledge organization. STCourse is a super vocabulary of science technology edited by NSTLCAS, ISTIC, CAMS, CES. Currently, STCourse has collected more than 240,000 agriculture terms and more than 90,000 concepts. STCourse is used in platform data retrieval and keywords extraction. Challenges. Agricultural information resources are limited. And China's agricultural information resources are the main resources. Data formats are varied and most are not submitted according to metadata standards. How metadata qualities, abstract, keyword, themes, etc. are often missing. Metadata attributes are low level of automatic and artificial intelligence. In data cleaning, metadata attributes, extracts, extents, and other aspects still need plenty of manual participation. Multi-legal problems. Metadata platforms include Chinese, English, and other languages. So it is difficult to achieve the unique field resource retrieval and data associations on multiple languages. Future class. Carry out extensive cooperation with small organization to realize the mutual information and resources. of agricultural information resources. Strongen the research and application of AI technology. AI technology in abstract extents. So it is difficult to achieve the unique field resource retrieval and data association on multiple languages. The future class. The future class. The future class. The future class. The future class. Carry out extensive cooperation with small organizations to realize the mutual sharing of agricultural information resources. Strongen the research and application of AI technology in abstract extents. Extraction. Automatic tagging of keywords, subject, class fiction, and so on. Try to achieve multi-language unified retrieval and intelligent recommendations based on every work. Strengthen the research and application of semantic knowledge of AI technology. And look forward to competing with our peers. This is the end of my speech. Thank you for your listening. Thank you so much for our panelists in Beijing. And I'm really sorry that she couldn't make it. In any case, if you have any question for her and for Babs. Sorry. This is. Yep. So if you have any questions, specific question for her, please don't hesitate to write it in the chat box. I could simply send this to them. And I would like to welcome our next panelist. This is. Medha Devare. She is a senior research fellow at IFPRI. This is the International Food Policy Research Institute. Medha leads one of the three models of the CGIR platform for big data and agriculture. Spurheading efforts to operationalize the fair principles towards fundable, accessible, interpretable, and reusable data across the CGIR's 13 centers. Thank you for being here Medha and over to you. Thanks, Ima. And thanks to the organizers for having me. Let me know if you can't hear me clearly or if you're something goes awry with the screen sharing. But you should be able to see my screen now, hopefully, correct? Yes. Okay, good. Good. So I'm going to launch right in. First of all, I'm going to acknowledge my co-authors who could not be here. So Therese and Pythagoras from SIO. And they have helped us greatly with this. And a lot of what I'm going to be talking about is in fact, thanks to them. Let me start in with an introduction like the others have given about CGIR, because I'm not sure how many people know that CGIR is a network of 15 research for development centers focused mostly in the global south. It is the largest global agricultural innovation network. And we, so I mentioned the 15 centers. We have a local presence in over 100 countries. Most of those countries with GDPs of less than a dollar a day. We've been around for most of, most of the centers have been around for about 50 years. And we have large networks of partnerships. We also have some of the work that we do is across our gene banks, which hold or are stewards of a very large number of crop seed, essentially accessions, germplasmation. So in a nutshell, that's really CGIR. What we focus on really is poverty reduction, particularly in rural areas. We work on issues related to food security and nutritional security. And we try to do that in, in a sort of a, an environmentally sustainable way with regard for natural resource management and ecosystem services. So, so that's just to sort of set the stage a little bit on what CGIR is. As we work in our centers, what we find is, as, as everybody here has, I think alluded to is a, is an increasingly digitized and digital agriculture. So this is sort of a conceptual model of a, of a feature farm, not, not in the US or in Europe, but actually in, in Africa somewhere, or it could be South Asia. This is really where things are going very digital. Many of the speakers have mentioned AI, artificial intelligence and machine learning. I come from the big data platform where we have the technologies to be deriving insights and solutions from data, large pools of data, in fact. But to do that, we need to be able to find and access the data, something you've heard about in earlier presentations as well. We need to be able to interpret that data, not just as humans, but as machines as well. We increasingly need to be able to aggregate large amounts of related data sets. And we, we generally need to be able to visualize map and of course analyze that data to derive insights from it. So that's kind of the, the, the scenario that we work in. And for the big data platform, one of the things that we're trying to do is assume open, open access to our data, our research data by default, but focus on fair data. And I don't think I need to go much more into what there is. Other people have mentioned this. What I'm showing you here is sort of our kind of overall approach to doing this. This is a schematic that shows the, the organized modules efforts to make data open and fair. And, and, and, and so the heart of this is the guardian data discovery portal, which enables sort of wider discovery and use by Google data sets, for instance, and other search engines by external apps for farmers and policymakers. And, and, and it does that by harvesting metadata from a number of different repositories or data platforms. And by data here, I mean both publications and data platforms. So for CGR centers, we have 15 centers, as I said, that amounts to 30 odd repositories because each center typically tends to have, unfortunately, a one data repository and one publications repository typically unlinked. So you don't know what publication was built using what data very often. So that's 30 repositories. So that's 30 repositories. We're also making data discoverable from USAID's digital data library from what used to be DFID. And I can never remember what that FCDO stands for, but it used to be DFID. The, the, the, the British equivalent of USAID roughly speaking. We have some data and an increasing amount of data and publications from the World Bank. We also make discoverable what Aaron talked about earlier, USDA's Ag Data Commons. And we have the Genesis platform from our gene banks, which I've already mentioned earlier. We also have data and publications visible from the government of India's open data portal, focusing primarily on the Ag part of that, because there's much more there. And there's a data verse sitting behind this for those who want to upload their data. And for that, we have built workflows to comply with, based on ontologies and control vocabularies that form the standards. So these workflows, the FAIR workflows are intended to generate data that's, that's consistently described in terms of the semantics of the data. And then we're also developing digital tools to collect data that's already standards compliance, already built on ontologies and, and, and such. And of course, the, the, the, the primary piece of all of this is the, so what you can find data. So big deal. And here, what I'm showing you is the, the sort of pipelines that we're building out to be able to analyze the data, to visualize it and explore it, both semantically as well as visually in, in, in maps and being able to query it that way as well. So just a quick view of what this looks like. This is a search done in Guardian on, on the terms here on nutrition women. The idea is I want to find data on nutrition as it affects women. So I go to Guardian, I type in those keywords. And here you see a quick grab, which you see data coming from three of our CGI, our centers, if pre, ul, re and sip, as well as FCDO, which used to be different. And you would say it's DDL data.gov.in, and you may not be able to see the bottom one, which is the world bank. There are a number of publications and datasets that are being discovered here. There's different ways to filter across the top and, and to, and to look at that data here. And of course I did, I did mention USDA Ag Data Commons Gene Banks. If you go further down this, you're likely to find datasets from, particularly from Ag Data Commons, not so much from Gene Banks here. So to do this, to, to be able to enable this sort of view, you, you need somehow to harmonize a number of different metadata schemas. And this is where a lot of the work that, that Pythagoras and Soterios do particularly for Guardian comes into play. So Guardian enriches the native metadata that comes in from the repositories where the data sits to enhance their fairness, particularly focusing on the findability and accessibility. So here's an example. This is, this is from one of our centers. This is a publication from one of our centers, CIMIT. And, and here what, what, what, what was done when you look at this particular, the same publication as it's viewed in Guardian. What, what's been done is a sort of a standardization of the, of the first name, last name format of the authors from what you see here, which is kind of, you know, mixed. It's, it's not a consistent format to a much more consistent format. And then also a deduping. So, so this, this same day, the same publication is available both from the, from the different repository as well as CIMIT's repository. We deduped it and made it, you know, consistently available just from this one place. If you go further down in that same publication, you see a set of keywords and these keywords are, are both keywords that have been input typically by, by the, the information and data managers. But as it's been enriched as well by pulling from AgriVoc as well as ontologies. And so that's, that's something that's very powerful and enhances the, the findability and accessibility of data. This is another example, which we chose you here, a dataset from, actually not a dataset. It's a publication from Icriesat, one of our other centers in India. And here you see an enhancement of, of the resource using geo names. So it's, it's, it's, this was something that was absent in the original publication sitting in the repository. So it's, this is a, the metadata enrichment, algorithms that are used by Guardian allowed this to, to happen using calling on geo names. And the last example I want to give you is, is sort of, again, the keywords that are enriched here. This is a dataset now that's, that's an Icriesat dataset. Again, the center that's based in India. And at the bottom of this, you see a keyword cloud, as you did before. In this case, the, the, the words here are drawn from both AgriVoc and ontologies. The earlier example I showed you was AgriVoc. And, and where the ontologies particularly come into play, even sometimes they're behind the scenes. They're not necessarily part of this cloud. But, but they very much play a role in making data further discoverable, say to Google data. So it's, it's quite important in, in, in making the data fair, not only within Guardian, but much beyond that, increasing the, the, the discoverability of data more widely, data assets more widely. So this is my last slide. I just want to talk quickly about some of the challenges that the team has faced, this IO team and, in enriching, in, in, in using these kinds of approaches. And of course, one of them is, is harmonization in the, in, in the harvesting and the harmonization step. So, here for CGI are particular, particularly we, we do use a consistent metadata, data schema. That's a sort of a, a, an extension of Dublin core. But there are differences in interpretation of labels. And even if the metadata schema used by our, our partners who we, who we harvest from maps to, to, to Dublin core. It, it, it, it is a difficult proposition as has been referred to in earlier presentations. I talked a little bit about deduping records. So that can be sort of a, a, a, you know, we have to be careful about doing that as well. The quality and the richness of metadata is, both of them are quite highly variable. So, so the, you know, for instance, for, for, for, in, in some instances you might see words. In some, in other instances you see something close to sentences or paragraphs in the keywords field, which makes it a difficult proposition to know how to deal with. And then of course there's in, in, in, in some, for some records the, the metadata is quite rich. And in others it's, it's very poor. There, there are, you know, just the most basic metadata available. So they're there, the text mining approaches that are used by our algorithms are become even more important. With automated enrichment comes a sort of a pitfall as well. So there's the potential for misinterpretation. For instance, I came across a, a data set of spatial, a data set about spatial data. And they talked in the abstract about grid cells, but that was being interpreted by our algorithms as cells. And so when you clicked on the, on the cells in the, in that cloud, in the, of the keywords, what you got, what you saw was a bunch of stuff on organismal, you know, the biological equivalent of cells, which is not what this was about at all. So, so there's a danger there as well. And then the, the, the one, you know, does one size fit fit fit all? And it doesn't typically because here we're talking largely about a variety of scales, all the way from the genetic to the landscape. And in terms of disciplines, all the way from the genomics, or the omics types of disciplines, to socioeconomics, to spatial data. And there, you know, it's, it's always a sort of a tussle. And, and compromises are involved in terms of the, the metadata and enriching that, of course, has its own special challenges and pitfalls, as you can imagine. So that's pretty much all I wanted to say. I wanted to give you a flavor of how we approach this at CGIAR. And I'm happy to take questions later on. Thank you. Thank you, Medha. Excellent. So we move to our last presentation. And this is, it's going to be delivered by Laura Meggelato. She's the team leader of the Land Portal Foundation, and she's responsible for the overall management, implementation and expansion of the Land Portal. I will be passing the slides on behalf. Laura, just let me know, Laura, when you want me to move. Thank you, Emma. Can you hear me? Yes. Thank you so much. Thank you for this opportunity. First of all, I just wanted to say that the, introduce also my colleague Carlo Tejo, who created these slides with me and we will be presenting together. Thank you so much. So next slide, please. Let me introduce quickly who we are. The Land Portal Foundation is a small Dutch, no profit organization. We've been working over the last 10 years to address the extreme fragmentation of land data, providing free and open information for a rather small community. So the land sector, the land governance sector is a very specialized sector within the agricultural domain. And according to it, so they have very special needs. So according to land researchers and practitioners. So access to land is key for nutrition and food security and increases investment agriculture productivity improves people live food. So, so access to land is at the heart of this small specialized community. I'm not, I mean, our, our, our numbers are much smaller than those of my previous colleagues, but I just wanted to let you understand what, what, what audience and what community we are serving. So when it comes to access to data and information, it's proved to be very close, close sector, the vast majority of land governance data remains closed, fragmented, poorly organized with poor metadata and many times with no metadata at all. So they're representing, if you search for land information on the web, you, you can find only few information providers. So this means a very narrow range of perspectives and content to keep to be published in a way that is not, doesn't facilitate discovery, discovery engagement in reuse. So because land is a cross cutting issue and very politicized. So analyzing land governance often means looking at not only different data types, but also different across different domains. So it also involves looking at different data stakeholders. stakeholders. So not only academia. So research data, but also the data provided by governments, civil society or private sector. Data providers. So different with different degrees of data capabilities. So our response to this extreme, extremely fragmentation and accessibility of information is twofold. So on one hand we develop and nurture a multilingual platform, Lamporza.org that hosts wide range of organized country issue data set pages where we aggregate and pool content data. And then also change data systems, dashed reserve access systems of different types, such as Anya- solicitations of different types such as bibliographic data or statistical indicators or special layers. We create a repository of experts and sector organizations, media content, especially from Southern countries. And on the other hand, we try to build capacity data capacity in the sector and also a data infrastructure. So in a few years the LEN port has become a leading online destination for data and information in the sector. So we have three main systems. We have a geo portal that aggregates spatial layers, a Drupal website that aggregates bibliographic data and media content and a triple store for statistical data. For the purpose of this presentation we will mainly focus on the LEN library. That is a Drupal instance where we maintain and create a repository of LEN governance related resources pulled from multiple sources in which the metadata map to semantic standards. So next slide please. This is our LEN library. So it includes again for us it's a big number 61,500 selected resources but I recognize that is a very small number compared to previous repositories. But is this content is either published directly by our community re-users or ingested through automatic importers. Most are metadata. But sometimes we upload the PDF, the resource as well. So as you can see is a very small repository that select content for a specialized community of experts speaking different languages from different geographies. So we pull resources from a number of different sources and improve the quality of native metadata. And at the same time we also encourage partners to publish metadata and better metadata. So in our library we adopt a customized version of the meaningful bibliographic metadata M2B to provide users even more information about the resources that exist in the library. So this custom adjustments to the M2B include for instance making some fields mandatory such as the subject field. And mainly because we also create a semantic set of controlled vocabulary online which is part of our vocabulary. I will explain in a few minutes. So we also assure the quality of this metadata by including license, language, geographical focus. That is very important for our audience. And for the value of the metadata fields we use standard codes such as the ISO 3166-1-A3 code for countries or the UNM-49 code for areas and regions. And as I said before for the subjects we use AgroVoc and in particular a sub schema of AgroVoc called LandVoc. It is a small 300 set of keywords or concept highly relevant to the small sector that we serve. So we contributed to developing LandVoc and maintaining LandVoc within AgroVoc. Up to my colleague now that explains the data flow. Thank you very much. Thank you very much. So in the next slide. So here in this slide I want to show you how data flow when we import information from time parties into the Land portal. So at the beginning we have the sources. Sources that could be very different from manual generated Excel file that follows a structure that is shared in the Land portal website. To a JSON response from CMS as WordPress. Or XML response from DSPACE. For instance DSPACE API. So we have this as the source. In the middle we have what we call the imported that are a piece of software. That mapping. They map two things. We need to map the fields itself. So a field in the source should be a field in the Land portal metadata. It could be some of them. And also we need to also to map the values. Because we are using several closed lists in the Land portal and we need to do some kind of mapping between the two values. And finally, in order to ingest this information into the Land portal.org that is a Drupal website. We use the feed important feature. And during this whole process, during the whole process, there are tasks of data curation. So if we see that there is an issue with the metadata or with a value, we contact directly our partners, our data providers in order to raise this potential issue. So, something that is important to highlight that it's in the Land portal. We are trying to not host PDFs. We are not trying to host the publications. We are always trying to point back to the source. And we only try to collect the metadata, not the PDFs. So in the next slide, we can see some of the challenge that we are facing. As my previous colleagues mentioned, I think that is more or less the same. That we have very different sources. So as we need to deal with different sources, some common challenge arise. So the first thing is to find relevant information. Because our sector is really small. So trying to find information is not easy. After that, it was okay. This source is interesting. They have relevant information. So what can we do? How many documents this source has? So if there are too many, maybe we can go to a machine approach. Working with some kind of software Python importers. Or maybe we need just to fill the metadata and spreadsheet and ingest it. And well, the CMSs that the content management systems that are behind the organizations, the data providers are very different in terms of the CMS itself. Or maybe it could be an HTML page. And also the version. And working with different CMSs and different versions make us to change the approach each time. And also the type of format, the data format of the source could be like RDF, CSV, XML, JSON files, RIS. Very different. Also the metadata mapping. We need to do one by one. Okay. This field, imagine that in the source we have a DC contributor. Maybe in our website could be or should be an author. We need to do this kind of mapping and also the data values. Another challenge that we have is the multilingualism. Mainly for the data curation. Because we can stamp more or less some languages, but other languages are out of the scope. And sometimes you don't know if there is a problem or not in the quality of the data. Last to mention is that we use Lamboc as for tagging the content. And not only for tagging the content. We are also using Lamboc. That is a subset of concept of AgroVoc. Also for harvesting in order to find relevant sources. So trying to map sometimes the source to this Lamboc is a challenge. And now I give back the floor to my colleague Laura. Next slide, please. So to conclude, we learned that what we learned over the past 10 years is this is a very challenging sector. Because it's absolutely fragmented. Data information is highly fragmented and enclosed in silos. So what we usually recommend to our data partners is to have an openness default mindset and follow the principle. So the capacity development is as important as maintaining and curating common open repositories. We recommend to use standards in data, any metadata, to share the metadata model, establish channels for continued feedback and collaboration with our partners. This is more a partnership rather than as serving an audience. But one of the most important lessons that we learned over the past years is that collaboration is key. So collaborating with like-minded organizations like those that are sitting in this panel today in the sector is very important. Joining forces to improve accessibility of scientific literature, promoting the use of standards, spread best practices, promote and build capacity development. And then also another key lesson is building on what already exists without reinventing things like reusing existing semantic vocabularies, for instance, and build communities of practice around those standards to help improving, constantly improving and promoting them. And this is very important. This is one of the most important lessons I wanted to share. Thank you very much. Thank you Laura. And thank you so much Carlos. So I think that we are, and thank you to all the panelists. I think that you did a great job. It's six to six here in the European Eastern time. I presume it's 12.06 in Washington DC. So thank you so much to all of you. So we can start now what we call the questions and answers. I haven't seen too many questions from the audience. That's I will encourage you once again, but if you have any, any kind of, yeah, question to, to, to, to, to any of the panelists, please use the chat box. And, uh, and, uh, we will answer as soon as possible. In the meantime, I have some questions for you, for all our panelists. And, uh, I was, uh, I would like to put them on the table and perhaps to have a kind of a round and to know a little bit, but all of you think about, um, I mean, uh, these, these topics, I think that in some cases you already mentioned, uh, how crucial it is. Uh, how crucial to recommends and to promote good practices is, um, are let's say in, in our domain, as much as it happens as well. And other domains, but what recommendations specifically you think that we could provide to organizations and to enhance this visibility of their resources through the improvement of the quality of metadata. What kind of steps would you suggest more specifically that we could do to help them or what you would recommend to them in Laura, you were very specific in your presentation right now. Talking about a specific community that you are targeting. So maybe we want to expand a little bit more. What do you mentioned before about this kind of recommendations that you are preparing for instance, workshops or, um, publications videos. Yes, of course. Um, as I mentioned, the building capacities, um, goes hand in hand with building the, the sector infrastructure. So, on one hand we work with the, the AgriVoc team to develop LandVoc as part of the general infrastructure. And then on the other hand we build capacity through, uh, MOOCs or workshops or tutorials for people to improve their, um, knowledge management capacity with their data management capacity. So we encourage them to, you to, to create metadata, to enrich their metadata, to expose their metadata in the right way to use standards in their metadata fields. Um, that's, and again, building a community that supports those best practices is, is very important. Um, from, from different, uh, sectors. So in the, in the, in the, in the LandVoc community of experts, for instance, we welcome, um, uh, leaders from civil society organizations or representative of government organizations or researchers from universities. So all with different needs and slightly different perspectives, but we want core, uh, idea in mind that the sharing data is very important, uh, to improve, uh, land governance. Thank you. Thank you, Laura. Meza, would you like to add something about, uh, lessons learned on these sites, uh, from experience with researchers and CGI. To respond to this part, you know, in the? I would probably reduce the use in their digitalization. In, uh, in the task. It's moving within the real説明 facility. You're active in background on these devices that invest in into in international outreach, or the microphone secluded and but in-room是我, uh. Good point. तンタᵉ That�로 Nielsen and I was also involved in with дивуж región, um, how are you doing that? It willам you all that? and information specialists across CGIAR, but more broadly even beyond CGIAR. So anything that we do will be open to others as well. Part of this focuses around capacity building, as Laura mentioned. A lot of it is also about culture change. So when you're working with researchers, you know, I was going to mention the third thing, which is tools and services, but really the biggest rub is the culture. And that's what you have to change. And to do that, there are multiple ways in which we are trying to do that. One is to build the capacity, to build the tools, to answer the question of what's in this for me, that the researchers need to sort of effectively understand and respond to. And basically to incentivize the proper annotation and the proper handling, the proper management of our data assets, you know, more broadly speaking. That takes many forms. It could be through, you know, how researchers are rewarded. Let's get away from the model where people are rewarded only for, you know, high impact factor publications. This is a message we're consistently trying to push out. How are you managing your data? How fair is the data? We've got metrics now to kind of, you know, put a quantitative stamp on how fair data is, so that you can start to now incentivize it more broadly through annual evaluations, for instance, of researchers, of scientists. You know, putting in place rewards, having data sprints that our data managers and information specialists organize on a pretty routine level. Hackathon, you know, cure-a-thons, they call them data cure-a-thons, and data sprints, where researchers are able to come in, spend two hours or half a day with very much hands-on, you know, help to make, to annotate their data assets better. So these are all pieces that we need to put in place, combined with the tools and the services, such as the fair workflows that I mentioned earlier, which will help researchers in the agricultural space particularly annotate their data consistently and very easily. The idea is to make it easy and, you know, let researchers, you know, be able to do that that way. So many different approaches, I would say. I see Erin agreeing with all the things that you're saying. Erin, would you like to say something else about that? Yes, that's the incentivization, just trying to convince people to produce better metadata is huge, because when you're uploading data files, if you don't have good metadata, you don't know what that is, and no one can reuse it, and a lot of times researchers, they're so strapped for time that they don't, it's an afterthought. Metadata is an afterthought, and then at that point, once they get their publication, they're just scrambling to go on to the next thing, and so we're trying to build it into the process so that it's maybe more from the beginning, like with the USDA, first of all, we are now required to have data management plans. So they have to start thinking about where they're going to put their data and how they're going to share their data and all of that, what they're going to produce, what standards they're going to use at the beginning before they even get the grant. And so that is, we're hoping that that helps to increase just their thinking about it, that they're thinking about it, they're coming up with that plan, and that it's built into their culture. It is a culture change, and you know, unless it's, unfortunately in those cases, unless it's mandated, a lot of people won't do it because they don't have the time, and they look at that metadata form. And then the other step is for us to make it more accessible for them. So they see a huge metadata form and they get intimidated. And we are also offering webinars and tutorials and one-on-one sessions and group sessions to help educate people and take the shock out of it when they see that. Right. And may I add just one word? What Erin is saying about culture is absolutely important. So the idea of sharing data is absolutely an attitude. And we noticed that people, for instance, they don't think about the license of their data. And if they don't put any license, that's closed by default. So, but they don't put the license just because they don't think about it, not just, not because sometimes they don't want to share the data. So it's really changing the attitude of how people deal with data and they don't think about sharing a data management plan before they start working on a publication. When they finish a publication, they think their job is done and they don't realize that if they don't share it, they don't make it open, then it's not really useful. Right. Fabrice, I don't know whether you would like, you had a specific slide about all these, but I don't know whether you would like to add something more. Fabrice, I would like to add something. I really agree about the rich, the asking for rich metadata and especially to make that available, but I want to add something about the technology. So I think that in this sector, we are very late. We are still talking about sharing the data and making data available while that should be given for granted. Tim Berners-Lee 20 years ago was talking about publishing your data, making your data available and so on. Now we live in the area of the technology. So what we should really think is not only sharing the data, but it's sharing the data in a way that machines can do a better job with the data. So for example, using standard metadata properties, of course, allows a machine to better understand what's going on without using very sophisticated algorithms that need training data sets and a specialized IT knowledge. Then for example, also relying on vocabularies, existing vocabularies or aligning your vocabulary with existing vocabularies can help, of course, connecting resources can help data discovery cannot data representation. So the usage of vocabularies, URIs, so once there are alignments, then we solve a lot of issues with understanding the meaning of the data with the possibility to link things together. And then when you publish the data, for example, on a website, the possibility to annotate pages with schema.org and using metadata so that search engines can understand what's going on in your website. Or if you have an API like document your API in a machine readable way or using tools like Swagger. So overall, I understand that we have 20 years of delay because we still have to face with problems related to making data available while we should address the next step, the next series of problems like making data available for machines. So to solve a lot of other issues that we have now. Right, right. Carlos, I don't know whether you would like to to say something about what Fabrizio was mentioning. I saw that you were listening very carefully or yes. Just I was thinking about we were talking about a lot about re-inverting the wheel all the time that we don't reinvent the wheel, but we're thinking more about not tripping on the same stone and we are facing the same challenge, the same different people in several places. So maybe something that to think about is to have a kind of session about what are the lessons learned and what are the actually the the issues that arises in in every kind of integration because I think that Fabrizio and I we get the same errors from different places. Sometimes if you have this lesson learned somewhere you will overcome this these issues faster and quicker. Right, so this brings me to another question that is more about a suggestion. So because I see that we all have the same problems and in a very similar way. So I'm just thinking what kind of next steps we could take as leaders of important information systems in agricultural sciences to join forces to really work on together to let's say overcome with these challenges or there is any activity do you think that we there is any kind of context where you think that we could simply make sure that there is this application that even Carlos was mentioning another area of it in general that we are finding ourselves in different areas could be shared lessons learned sharing standardization of data. I'm so I would have walked all over the place. So what makes me very happy since eventually I'm the Agravog manager and I'm working as well with NALT to math both vocabulary. So this is very very important at the moment, but there is any anything that you think and that we could work together. I know that for instance uh if I was working with Laura in doing capacity development activities we are supporting uh the LAM portal as much as we can also with I know we are working on this mapping I was mentioning with NEDA we are also working in some some areas related to mapping ontologies in CIR and etc etc. this one of some of the areas but do you have anything else in mind that you could you think that we could do together to improve this situation that we put on the table today? I have a thought about that and it actually relates in part to something someone just typed in the chat about dealing with rigorous standards and metadata standards for different subject areas and that's one challenge that we have we're dealing with genomics data and life cycle assessment data and geospatial data and it's all ag related but in the ag cross example that I showed in my in my slide may be finding ways to find where the subject specific people are keeping their data and finding ways to standardize it in order to make it more findable but then to point back so that they don't have to necessarily conform to one standard and I think Medha mentioned this in her presentation that we can't you can't make one one size fits all it doesn't and so what we should be doing is trying to figure out who's searching what are they trying to accomplish and then getting together and figuring out how we can facilitate that so whether it's using a standard vocabulary to integrate into one platform that people can search and then go off into their subject specific maybe something to something like that I mean we do that to a lesser degree I mean we're focusing on us United States data USDA data specifically but some of you are aggregating what we collect and so if we can feed into that I would love to know what would make it easier as you're taking in our data to make it more discoverable you know on a larger platform right okay do you have any other comments Medha, Fabrizio, Laura, Carlos? sure I mean one of the things that occurs to me is here we have you know some of the you know fairly some fairly important platforms in in agriculture and repositories and we could rope in a few others perhaps World Bank for instance in a couple of other entities but it would it would perhaps be helpful to together develop a framework or a set of principles that that we stamp with with our presence essentially with our organizational presence and and float out there as sort of guidelines along with attached tools that that people can choose from it it doesn't have to be your tool or my tool we don't have to choose among them but we we make make it possible for people to not only see what what best practice really might be or is really but but also how do I actually make it simple for me you want me to do this tell me how to do it and I'll do it you know that that sort of approach needs to be could could perhaps be helpful the other thing I would say is you know in terms of incentivization one of the things that we're doing that that I'm trying to figure out how this group could work together on is one of the things we're trying to do is show researchers what the value is of well described data what can data science do for you and so one of the you know one of the sort of the small projects that the big data platform funded through the organized module is you know answering the broad question of you know is fertilizer use really really profitable in Africa we all go with the assumption that it actually increases yields but does it actually translate to profit and if so where and so this was done using guardian they you know the researchers a team of three primary researchers found 200 data sets in guardian 760 locations and they were able to put you know use machine learning over this kind of data pool this these data points to derive some insight and and actually found that it's not always you know even where yields are very high profits can be very low because it depends of course as you might expect on on market value of the of the crop on on the price of fertilizer etc so being able to pool all of these data and spatially you know identify where where fertilizer is useful it's one of the key things that we you know that's that's a key question for for um african agriculture and and we can do that uh and and once researchers see this kind of thing being done they get really excited about it so can we then uh you know with these growing data pools come up with these kinds of use cases that would be very interesting i think in terms of changing hearts and minds right yeah yeah yeah right sorry oh no this uh this example is uh it's very it's very nice and i think that this is the last uh step of of the chain because as far as i see now there are two two two two different groups so there are the producers of the data and as we are saying we can oblige them to use a common standard a common format they produce the data as they can produce it what we can do as aggregators and agris is doing already that but we can always do it better is to do capacity development with them when we receive the data with wrong metadata and in the wrong places so for example there is the alternative title and you find there the journal title you have to tell them that this is this is not working so this is the the first thing that we can do work with the producer of the data and help them to produce something better all the rest is up to us so uh the harmonization of the data i mean the data enrichment we have to to do an effort sometimes we're going to do a joint effort because everybody has uh their own way to to to be funded so we are also guided by what we find outside but what we could do is to to work together to like recommendations that we can use uh that we can use all together when it's time to urbanize the data when it's time to give some not truths but recommendations to publish the data and to make that available because then at that point there is the third step that meda was saying and all this will help the usage of the data to discover to create new knowledge and to this to discover new things but as aggregators we have a responsibility to guide the data providers and between us even if uh there can be like uh political and institutional problems to to at least write down some guidelines that we can all follow well we belong to a very small um community so uh we always um we always um had major uh benefits from collaboration collaborating with other uh aggregators and within a wider sector like the agriculture sector i certainly welcome the idea of developing common tools or guidelines to facilitate to facilitate to facilitate interoperability and raise awareness um and and to me is is also a matter of making um i mean one of the incentives for instance in my opinion is really demonstrate what the consequences of a closed data environment are what a closed data environment will look like and why a more um interoperable uh data environment is is then more democratic so this is a kind of the um the argument that we bring to the table all the time says um making data visible interoperable means uh making the data landscape more democratic and diverse uh we are almost finishing the panel session but i just realized that there is a question for mida that's uh says can you say more about your metrics for how fair the data is yeah so um i i saw that and i pulled up uh i'm sharing my screen again just to show you how we do this um so so this is the the guardian landing page guardian.bigdata.cgr.org if you go to about guardian or if you do a search in guardian um every every resource you'll see a fair metrics uh link to and and you can go to sorry to analytics here and when you go to analytics you see something about um you know fairness across particularly across cgir uh but you can go to view metrics here and it tells you a little bit about how we're actually scoring for fair and and basically what that amounts to is that we're using um um if i click on that download the the the guide we've got a guide here a step-by-step sort of blow-by-blow guide on how you get to zero from zero to level five for both for all of these things for f a i and r um so this is what we've tried to use in in terms of how we score uh it's built on the on the net the netherlands uh archive for it's dance i can never remember what it stands for it's somewhere here permanent access to digital research resources dance uh roughly translated so we've used their metrics to to develop these algorithms and so each data asset in guardian is scored uh that way we're changing our our interface so you'll see something like this now with a very large data management toolkit and if you go to that to that toolkit um at the very bottom for instance you you'll see you can browse them and so if you go to data curation you'll see the fair data guidelines here and the fair data workflow uh will be available soon as well there in another couple of weeks thank you so much this brings us to the end of the session i would really like to thank to all the panelists for such a nice one hour and a half i have really enjoyed to listening to all of you and i hope that attendees also enjoyed this um this session as well as you have seen this is just the beginning of something else so we hope that perhaps in another time we will uh talk more about what we we are going to do together as we were putting on the table today so thank you to everybody thank you as well for the organizers of this cmi and for giving to us this opportunity