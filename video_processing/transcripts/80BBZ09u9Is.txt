Marcel Fafchamps: Mobilizing P2P Diffusion for New Agricultural Practices
https://www.youtube.com/watch?v=80BBZ09u9Is
Education
 I'm going to host. Welcome, everybody. This is the last of six seminars that we've organized as part of a webinar series on targeting and community networks and anti-poverty programs. And we're very happy to have Marcel Fachon give the last seminar. Marcel was the first person to teach me about development economics way back when at Stanford in graduate school. So it's always great to see him. And he's going to share some of his work on mobilizing P2B diffusion in Bangladesh. Marcel, it's all yours. Thanks a lot. Well, it's a great pleasure to be talking to you guys. This is a joint work with Assad Islam, who I believe is attending Abdul-Malek, who also said he would be attending, and Debyean Pakrashi. And it's a second paper we've done together about basically about how to promote the diffusion of agricultural technology and focusing on a specific technology called systems of rice intensification, SRI, which has the advantage and drawback that it doesn't include, it doesn't require additional input. So it's an advantage for a lot. So because you don't run into issues of people having liquidity constraints or things like that. But then the downside is that it's actually a difficult technology to master. And so people, the difficulty with this technology is how to train farmers in applying it. And so, of course, you could try to train all the potential beneficiaries or because, you know, if you don't know who's going to benefit from it, you can try to train everybody. But that's obviously going to be costly. And so with BRAC, we've been trying to, BRAC in Bangladesh, we've been trying to find ways of targeting beneficiaries, if you want, the farmers who would benefit from this innovation of targeting them better. The first article we wrote about that, which we tried to get, we trained some farmers and then we asked them to recommend, to refer basically, all the farmers they thought would benefit from the training. So that was the first step we did. That kind of worked and it didn't really work like the way we were expecting. We were expecting the referred trainees to be adopting, you know, to be better adopters, basically, be basically better targeted. And that's not really what we found. So this time we tried another approach, which is, you know, has a long history in agricultural extension, which is basically based on the idea of a model farmer. But we tried to improve it, basically. So, but if you remember the model farmer, the extension agent comes to the village and says, I can't, I can't, you know, teach everybody. So I'm going to, to select some farmers who I believe are role models for others, who are also more receptive to innovation. And I'm going to train them. And then I'm going to hope that this will then naturally diffuse through an unspecified process through the local population. Okay. I'm sure they are more structured ways of doing it, but that's kind of a rough characterization of what the model farmer approach is. And so what we tried to do here is we're going to try to start from that. And, but instead of hoping that these, these model farmers, you know, do viral marketing, basically, they basically become your, your champions, your marketing agents for your, for your, for your innovation. We're going to actually ask them to train two farmers. We're going to assign them these two farmers. We're going to say, okay, you're going to train Jack and Jill. These are the two, the two trainees we've assigned to you and you go, you know, try to do a good job of it. So that's the, that's the basic idea that we have. Our intervention is basically taking trainees, people who have received this one day a course on SRI from BRAC and we're going to assign them to other farmers who could have been trainees, could have been BRAC trainees, but we're going to say, well, what do we get, can we get, can we, what, what fraction of the effect of BRAC training can we achieve by asking this, these trainees to themselves teach other farmers? Well, that's, that's the logic. So each, each of the, so we call them teacher trainees. Sometimes we call them teachers for short, but obviously they're not school teachers. They just, uh, asked to teach other, other farmers in our experiment. Can I ask a clarification question, please? Yeah. So how much, uh, experience do these teacher trainees have with the technology before they're expected to train others? No. They don't have any. They don't have any. Experience with the technology. So you said experience. So they have been, they received a one day training from BRAC and then they are asked to, you know, can you teach the, the instruction list that you, you have been, you know, told and, and, and talked, talked about, uh, to these two other people. Yeah. Yeah. And people take a quiz. And so basically we're going to be, we're going to be comparing adoption between teacher trainees and trainees. Of course we also have control farmers who didn't receive any training. We have control farmers in the treated villages where some people received the training. We also have control, uh, farmers in villages where there was no training whatsoever. So I'll talk about that in a minute, but that's the, that's the way we're going to try to see whether, um, whether this model, uh, transfers knowledge. So we're going to quiz the, the, the trainees, the teacher trainees at the end on, on what they're supposed to be learning. And we're going to teach the, the students of these, uh, teacher trainees who did not receive the BRAC training. I'm going to, I'm going to quiz them about, about their knowledge as well. And then we're going to see whether they have, you know, we, we also have, uh, we know that, uh, the average farmer who has not been trained does not know what SRI is in this particular part of the world. So it's not, it's not like they would not naturally know what it is. And then we also look at adoption, um, of the recommended practices later on. And we, uh, which is measured by BRAC. And then we also, um, have a, an end-line survey where we measure, uh, agricultural outcomes, like profits and yields and so forth. So that's the, that's the basic, uh, idea. I'll, I'll go through the, to the intervention more detail in a minute. Um, there are some villages we incentivize the teachers to basically we thought maybe if we incentivize them, they could do a better job. So they are incentivized to teach, um, to the test. So basically they, they, they've already taken the test and they are told, well, you will test your students, uh, using the same quiz. And then we'll, we'll give you an incentive if they answer the quiz questions well. So that's basically testing whether they can, um, they can convey this key information about SRI to their, to their students. And, uh, we hope that that knowledge and translates into, into a higher adoption adoption. And then, you know, from the model farmer idea, this model farmer idea. So remember the, the, the, the key idea of that model was that this person was a role model for other farmers and therefore the other farmers would want to copy him or her. So we thought, well, maybe we can emulate that as well. I mean, here, so the way we did that is that we ask in baseline, we ask each of the farmers in our sample to, to list at most five other farmers in, in, in, in the list we've given them, um, whom they would regard as, as role models. So, um, whom they would basically, whom they would consider as sources of valuable information for, for agricultural technology. So that's the, that's the idea. And, um, and then basically in a way I'll explain where we, we, we, you know, remember each teacher trainee gets two students assigned to him, they're all male. Um, so, and one of them is going to be someone who has, who has nominated that farmer as a role model, who has listed him as a role model. And the other one is someone who has not nominated him as role model. So that's the idea. So, and the idea is based on this model farmer idea is that it's a role model. Therefore it's going to work better, uh, in terms of diffusing the innovation from teacher trainees to those farmers and nominated them to, to those farmers that regard them, regard a teacher trainee as a role model. So that's the, that's the key idea. Um, and, and this is done through an algorithm I'll describe in a minute. So that's the basic idea of what we do. Um, in terms of, and I'll, I'll explain that the experiment more, more detail, obviously in terms of, uh, just to remind, I'm sure some of you already know, uh, what, uh, this system perhaps even better than I do, what the system of rising intensification is. But, uh, my understanding of it is basically, it's a set of different practices that have to do with transplanting, the way you do the transplanting, the date of transplanting, and especially things like spacing, thinning, and, uh, the way you water the plant, the regularity of the water in the plant. And so on. So this is space. They have to be basically spaced further apart from each other. And, uh, and there's going to be more thinning than what farmers would normally do. And as a result, these, uh, these, uh, single plants that survive grow bigger and they get, they get more grains. So that's, that's the logic of SRI. That's as far as I understand it. So, uh, now you can see there's no, you know, these are all practices. There's no, um, inputs required. So there's no, there's no purchased inputs. I mean, uh, that are related to adoption of SRI. So that, that means several things. Um, it means things that, um, and if it was a fertilizer or a new seed or a new pesticide or something, farmers could in principle go to the, to the seller of this, uh, input and ask them, you know, what should I use on my rice? How should I use it? How should I apply it when there's no such thing here. So we don't have to worry about contamination of our sample by some external agent who comes and gives them, uh, information about this technology. So that, that simplifies our work. It's, it makes, uh, eliminate, it reduces contamination from external, uh, actors or vendors in this case. Um, it also means that you don't, you don't have to worry about externalities through input markets. I mean, if you, if you introduce a new, uh, a new, a new fertilizer, let's say, and, uh, there's a whole value chain has to bring this fertilizer to the villages. And, uh, only, you know, nobody, only you are the only one who wants to buy it. You know, chances are you're not going to find it because the, the traders are not going to find it useful to bring the fertilizer to the village, just in the off chance that you might buy this one bag of fertilizer. So they it's, so they're going to be network externalities in the sense that, you know, the more people are buying it, the more it would make sense for, um, uh, sellers of, uh, agricultural inputs to stock it. And therefore you would have, and you will have more information. And the other farmers will have more information. So you get into this kind of system of, uh, of externality. So we don't have to work at the prime with that is that that would break Siddha. So, so, I mean, uh, we, we already looking at diffusion, which is something that's very already by name, by nature kind of breaks, breaks Siddha somewhat. So we, we, we don't want more to have to worry about more violations of Siddha, uh, that, that we have to. Could there be an effect on the quantity of inputs that they purchase? I'm wondering if you have to space out. We will look at that, but we're not introducing any new inputs. Yeah. But we'll look at that. Yeah. Um, the, the main thing though, is it's, it's, I would, people sometimes describe this technology as more labor intensive. It's, it seems like we, because, because the farmers have to go and visit the field sometimes more often, but really it's more management incentive intensive. They have to be more careful and they have to also change the way they were doing things, but they were, they have to measure, they have to count how to thin properly and so on. So they, it's a little bit more scientific. And so as a result, it's a bit more demanding in terms of attention and care and, and a bit harder for the farmer to delegate. So, uh, that's a sense in which it's more demanding on the farmer. Well, that's the idea. So, but these things again, means that if it's new, it's about management, it's not something that will diffuse naturally through observation. You can't just copy people. Oh, he has a new iPhone. I'm going to have a new iPhone. You can't just copy people's behavior just by, by observing what they're doing. So you have to really be taught how to do it. So that's the, that's the idea, uh, of this. There's going to be some diffusion happening and naturally, of course people will talk, but it will come, you know, the hope is that it all comes from, uh, from the training that, uh, uh, BRAC does. And then the way that this diffuses then to students and perhaps then eventually to some other people in the village. And as we will see later, there's very good common contamination across villages. So, which again, confirming that people don't seem to know this technology beforehand. Um, now I, I'm not going to spend too much on this slide, but, but there's, there's obviously a very large literature on diffusion of innovation here. I've picked a few, uh, papers that, uh, that are more in the development economics literature, reasonably recent papers. Um, so that, but it is of course a much larger literature behind it, uh, including of course in marketing, uh, because, you know, diffusion of new products, these are innovations. So there's, there's going to be some, some large interest on that. There's so many, but you're on SRI and basically the base, the basic, the main message of this literature is that SRI is not for everyone. You, you need to be a dedicated farmer to benefit from it. If you're not, um, up to snuff, let's say you just like, uh, somebody who does it because they can't earn an income from some, from something else, you might not necessarily benefit from it because it's actually demanding in terms of, of, of understanding of what's going on in the field. So that that's the sense in which it's not a technology that we expect to benefit everyone. And therefore it's a technology for which targeting is important. You know, you have to be able to, you know, you want to economize, uh, on the cost of training by focusing your training on, on people who will benefit from it. So that's the idea. Um, of course there is also a larger picture on peer-to-peer transmission, which is what we are trying to, uh, achieve here. And of course, more generally on the role of social products, proximity in, in viral marketing. So I'm not gonna, you know, these are just a hand, just very small number of papers I'm citing here. So let me, if, if there are no more questions about the, no questions about the, the setup, the context, I can delve into the, immediately delve into the experimental design. As I said, it's with BRAC, um, we've selected a hundred villages, uh, from these two districts, uh, Rangpur and Bagura in, uh, Bangladesh. Um, and the way that BRAC works, um, as far as SRI is concerned, they go to these, when they arrive in a village, they start by identifying, or 30 potential SRI adopters. And these are people who obviously have to grow rice, otherwise there's no point. And, um, they have to, they have to be large enough. In other words, more than one decimal. I, I forgot exactly the, it's in the paper, but the, the lower limit, but they tried to find these intermediate farmers who have enough, uh, uh, land that they grow to, to, to, to signal that they're actually farmers, not just, uh, uh, gardeners if you want. And, but they also not too big. Okay. So they, they kind of live in this intermediate, uh, size. So they kind of, uh, generally poor people, but they are a little bit, uh, more focused on rice than, than some of the very small ones. Um, and if the village is too big and they have more than 30, then basically what they would end up doing, um, uh, not necessarily here, but what they would end up doing is kind of, they say there's two groups of 30 and then they would like, one in the eastern part of the village, one of the western part of the village. But, you know, it's a densely populated country. So you, you get, uh, sometimes you get large villages. So they would do that. So basically they, and that's, that's how they proceed, um, in their SRI training campaign. So we, we wanted to, to kind of, we didn't want to change that because we want it to be policy relevant. We wanted whatever we, whatever we introduce, whatever we, we, we find, we wanted this to be immediately, you could slot it into the normal strategy, the normal practices of BRAC in the country. So that was the idea. Um, and also we, you know, because they're not stupid, they, they, they also want to save on, uh, we economize on, on, on train waste, wasting training time on, on, uh, people who never adopt. We hope that through that system, we, the 30 farmers, they identify are probably farmers among whom the probability adoption is higher. It's not a hundred percent by any means, but it's, it's, it's higher. So that's the idea. So basically, uh, let's focus on those people. Um, um, and we're going to be looking at diffusion among that group. We start with a baseline questionnaire on household composition in pharmacies. It's a fairly light, um, uh, baseline. Um, and then we assigned 60 villages to treatment and the 40 remaining ones to control. In controls, there's no, uh, SRI BRAC intervention. And then in the treated villages, um, a selected number of teacher trainees, uh, farmers, receive one day training in SRI. Okay. And, uh, um, of the 60 villages selected for treatment, there's 30 that are also assigned to an incentivized teacher treatment assist, uh, and, uh, um, a treatment whereby the teacher train trainee, um, is, is gaining, uh, a compensation for training students that depend on their performance in the quiz. While in the other 30 villages, the teacher trainee receives a fixed, uh, compensation per, per student. And if you look at the, you know, more precisely, what is the, how does this compensation are constructed? Well, they, they get a fixed fee, if you want, a participation fee of, uh, four, 300 TACA or $4. And then in the incentivized treatment, they get 250 TACA additional per student at the end of the teaching week. And then another, uh, they get, they are given a week to train them because I know I'm expecting them to be training them, uh, every day all the time, but you know, you have to give them some time for them to meet them. So that's the logic of it. And you don't wait too long. So there's no point. Um, and then the incentivized question. Yeah. Yeah. How are the, how are the teacher trainees selected? I, I, I'm getting to that. It's the next, getting to that right, right away. Anyway, so that's the easy part explaining the, this kind of, uh, incentives is basically you get a fixed amount and then you, you lose something every time the, that the, your student, uh, answers the quiz wrongly. Um, so now let's, let's talk about the assignment rule. Um, so as I explained earlier in the baseline, in the treated villages, each farmer nominates up to five as role models, but certainly opinion leaders overall models. And then we have these 30 farmers and then we, we have to assign them into, to, to four different roles basically. Some are going to be teachers. They're going to receive the training. Some are going to be receiving nothing at all. They're going to be the non-students. They're going to be, uh, going to be able to look at, if you want, and, uh, un-incentivized, unorganized diffusion. And then we're going to have the students and they have two types of students and students who nominated. So remember there are two students per teacher trainee, one nominated a teacher trainee as a opinion leader or role model and one who did not nominate that particular teacher training. So that's how it's, uh, so they have to be assigned to these four groups. How do we do this? So we start by picking, uh, six teacher trainees. So those are going to receive the BRAC training and, um, this we did, um, uh, by dividing. So remember once we get the five nominations per nominating farmer, per, per, per, per each of the 30 farmers, we can look at who they nominate, right? So maybe all of them nominate Albert. So then, you know, Albert will be nominated by, you know, we'll have 29 nominations and then, uh, you know, maybe Dilip is going to have 28 or something. So you, you get a sense of, and then some people will have none. But basically, we basically divide the, the, the 30 farmers into those that received, uh, above the median number of nominations and those that receive less than the median number of nominations. So that's the, and then among the, uh, those that are above the median, we, we'd select four teacher trainees and the others we select two teacher trainees. So this is basically stratified sampling, right? We had a variable where we collected ourselves. It's known to us. We do sampling stratified. Okay. So they don't, they don't have the same base on their observable characteristics at baseline. They get slightly different probabilities of being assigned to a particular treatment. Okay. So we'll have to correct for that when we, in a minute, because that, that, that, that's stratified sampling, which is not pure randomization. Now, what about the other 24 farmers? So then we have assigned these six people at random, uh, uh, uh, the way I explained. So you're left with 24 farmers. We know we want 12 students and 12 non-students. And then we want, uh, six students who are matched with a teacher that they nominated and, uh, six students who are matched with somebody they did not dominate. So how do we do this? Well, we do this using an algorithm. This algorithm is a mechanistic. So it just basically you can think of the, of the algorithm as, um, you start by randomizing all the farms, the 30 farmers in the village. Then you, you know, based on that sorting, reshuffling of the farmers, you pick the four, the first four who are above the median in terms of nomination and the first two who are above the median, uh, below the median in terms of nomination. These are going to be the teacher trainings. So now that you have the teacher trainings, you start looking among the, the, the, the 24 farmers for farmers who nominated them, farmers who did not dominate them, and so on. And you go through the list, you go through the list of farmers in the order they've been shuffled until you finally allocate everybody. So the, the order in which they've been shuffled will determine the, the order in which they are considered by the algorithm to be assigned to a teacher or not. Does it make sense? So that means that we... Marcel, it seems, it still seems like it's not going to be very random if, uh, for each of the, uh, teachers, you have to find one student who nominated them. Because maybe for some of the teachers, there was only one or two students who nominated him, right? If I'm understanding the algorithm correctly. Marcel, yeah, yeah. But that's enough. That's okay. That's okay. Yeah. I don't think there is a village where we've, we had the problem of, uh, of selecting, uh, a teacher who was nominated by nobody. I don't think it happened because I, this is an issue that we raised at the beginning, but they, they just didn't occur. Okay. So you always find they'd be nominated by at least one person. Uh, one other question. I mean, when they are asked to nominate somebody as a role model, are they, you know, one criterion could be who you, who do you think is going to be the most successful in adopting the technology himself? The other is who is going to be the best teacher? Uh, and those two need not be the same. So what, what is your understanding of what, you know, what they think of when they say role model? Um, well, I mean, you will see it actually, this has no effect. Okay. So we, we can talk about what they meant. Maybe they didn't mean anything. It's possible. Um, but, uh, the wording was basically whom would you be looking, I mean, the word, the wording is not in, it is phrased in terms of agricultural technology. So who, who would you be going to for information about the new agricultural technology? That's the basic idea. Now it may be that they, they just go to their neighbor. So it may be that this person is not particularly knowledgeable about it. Yeah. But that's, that's, that was the intention. The intention was to identify someone who was perceived as a, as a more, at least by, by, by the student farmers, by perceived as someone who would be more authoritative. That was the, the, the intention. And so the idea was that if he's more authoritative, you, you probably would, would listen more to what he says. You might do better on the quiz. And you might also, um, follow, um, this person's example more. And therefore, and therefore, um, if they adopt, you might adopt it, uh, adopt this work. In fact, we, we're going to be testing for those things in the, in the paper. But before I go to that, to the, for the most substantive issues of the paper, which are actually where, where the, the interesting stuff is, um, I need to be, I need to basically address this issue of, um, weights. I mean, clearly this algorithm, um, generates different probabilities of being assigned to one of the four treatments for each of the 30 farmers. And, um, and so you might think, well, wait a minute. I mean, I know how to deal with this problem if it, if it's stratified sampling, but it's a stratified sampling. I just look at the stratification ratio that I use and I correct for that using weights. But here you say, well, wait a minute. I don't, I don't know. Is this the algorithm? It's a black box. But, but the thing is that the algorithm, you can reproduce it as many, as many times as you want. You can reshuffle the farmers, write it again, reshuffle the farmers, write it again, write it again, write it again, write it 500 times. Look at how, you know, of these 500 times, how many times does Albert get assigned to be a teacher and how much does Marcel get assigned to be a teacher? And that, that becomes your probability of assignment. Right? The problem is going to be, if there is somebody who's always assigned to be a teacher, somebody who's never assigned to be a teacher, then you, you think, well, wait a minute, those people I can't compare because they, uh, you know, they, they, I don't, I don't have people who are, uh, um, so I, I will look at that. I'll have to check that this is not the case because those people that would have to be taken out because they, they never, they have zero chance of, uh, you can't construct, you can't construct, uh, uh, uh, uh, uh, sampling weight for them. Right? It's infinite, basically. Or zero. So that's, uh, we, we, I'll get back to that in a minute, but that's the, that's the idea. But the main, the main, the main thing here is that it's going to rule, rule-based determinants, like an aspect given a certain ordering of farmers in reshuffling of farmers, and it follows a rule. So it's, and this rule is entirely based on things that we observe at baseline. Just like if you were doing certified sampling, you have some kind of baseline information on the people, and then you're going to organize your sampling based on that. So it's not, it's not about selection on unobservables. They're going to be things that are correlated with these observables for sure, but, but, uh, we, we, that's why we need to correct for, uh, for sampling weights, but, uh, so that's the idea. Um, so as I said, we have got four types of treated farmers, um, two types of treated villages, incentivized teacher and non-incentivized teacher. We only have one type of control farmers. And basically what we'll do is we compare, say teacher trainees to control farmers or teacher trainees to non-students or, um, so that's the, that's what we're going to be doing. So let's talk a little bit more about this assignment treatment type. Fine. So remember this assignment treatment is under our full control. There's no, there's no self selection whatsoever. We assign them. It's based on observables, which we have at baseline. And, uh, it's, it's, it's based on an algorithm, which we can repli- use to replicate many times the, the, the, the, the, the, the assignment process. So that's the key. But basically we can, we, based on that, we can, as I've explained, we can reconstruct what the, what the probability of being assigned to the, to each of the four treatments, uh, would be for any particular, in fact, for any individual in the sample. You can do this individual by individual. Okay. And so that, that way we can recover the polarities. So let me show you just how it works in practice. So let's say you have two populations that are treated and the control, the, the normal way of measuring the average treatment effect would be just to look at the mean of the treatment, the mean of the control, if they have been randomly assigned to it. And if there's, it's, there's no clarification whatsoever, you just take the, the sample mean. Okay. Now, in our case, we can't do the sample mean. We're gonna, for the, for the, for the, for the, so this is, this is people assigned to a specific treatment. So we're going to, to basically use inverse probability weighting. That's, that's something you already know. So, you know, we, we, we, this is the pro, this is the, the probability that somebody would be assigned to this particular treatment. We normalize this by case or which is the sum of these, uh, uh, PI so that you want to normalize because you want this thing that is say, the probability is one or everybody in a particular unit. You want this thing to be one over one. Right. So that, that cancels out and you, you're back to the standard, uh, mean. Everybody happy with that? You need to standardize it, but you don't, you don't bias the, the mean above over. Marcel, I had a question. I mean, this all makes sense to me, but if you were going to re-weight to try to make it in some sense representative, then why don't you just randomize at the beginning Marcel, I mean, and then you don't have to deal with the weights. I'm just curious about, you know, the decision to stratify in this way and then to re-weight it back to the random representedness. No, but no, but the, uh, no, that, that is, uh, why do you stratify a sample in any, any, you know, when you ever stratify a sample? It's because you want to be able to compare two, two populations. In this case, we wanted to compare nominating and non-nominating. We wanted to compare uh, teacher and the others and we wanted to be, um, and the big, the big challenge was nominating and non-nominating because we don't control who's nominating who. So basically that's the, that's the way with we, uh, we ended up approaching it. Right. But the cost is you get a higher variance. And I mean, the greater variance in the weights reduces power in some sense, right? So, yeah, but I can see the trade-off, right? No, I understand. I agree with that. Yeah, I agree with that. But we, we will have enough power. So, so my interpretation is that you wanted to try to understand the mechanisms as well. And if you're trying to think about mechanisms, uh, I mean, what I would be inclined to do is to, well, there's obviously heterogeneity. Let's say that people vary in skill, which, and there's just essentially one kind of skill, you know, you're better role model or not. And there's information about your skill that's in the, in the population. Uh, so you ask people and, uh, uh, you know, to nominate people. So you try to elicit community information about skill. Uh, but you've got that fundamental heterogeneity and there could be a second order of heterogeneity, which is that the effectiveness of, uh, you as a teacher also depends on the skill of the student. So it could be math specific. So what you want to do is estimate, you know, for any pair of skills for the teacher and the student, you know, how effective is the transmission of diffusion going to be. So that's what you're going to try and back out. Uh, then, uh, you know, I presume that this, this design will get at that, uh, because you've randomized and then you've got a third dimension of randomization, which is the, uh, the incentive for the, the, the teacher. Uh, yeah. Yeah. That's my view. I guess you could have, you could have written down a model that would have rationalized the design and, and all the effects that you want to isolate. I imagine that that's at the back of your mind. Yeah. It's in the back of the mind. I, I didn't, for this paper, we didn't, we didn't write a model. I, I, I confess we didn't write a model. No, we just focused on the, um, I, yeah, we just didn't do it for this, this paper. Um, and maybe I should have added, I mean, there's some discussion of that in the, in the, in the paper, and maybe I should have added some slides about that, but I, I thought I was just going to talk, talk you into it as I was going, uh, I was, I was presenting the different pieces of evidence I have, um, but you're right. So, so you, you raised several points here. So there's one I remember, which is the, what about match specific game? So remember, I, I, maybe you, maybe you saw the paper I wrote with Dimo about that. This is, these are kids who are assigned to, to do a course, a math course with one of the kid in a Chinese school. And so there we actually looked at, at, at, at the math specific game, but then they, it was easier, much easier because everybody was in pairs. They were just assigned randomly by design to another person. There was no, there was no need for a matching algorithm like this, like we did here. It was just basically randomized matching. And so there we could see whether, you know, if, if a, if a less able kid is matched with a more able kid, whether, whether the, the, the less able kid learns more, it turns out they do actually. And the, the more able kid doesn't seem to learn less. So, so that was actually a good, a good result for that. So that's an example of, of, of, so I know, but here we are, we're just going to be looking at the average. So I'm, I'm not, I don't have, I don't, I don't ambition to identify pay-wise because I don't have very, as you're, you're right. I don't have very good identification there. So I, I don't, I don't want to make any, any statements about that. It's not that I don't think it's important. It's just, just, I don't think I'm very, I'm on very strong terrain there. So whether avoid seeing that. Um, I suspect you, the way you stratified, you have the identification, but you may be not be having enough power. I don't know about that. Well, we'll see about power. I, I think we have power and we actually, um, cluster standard errors at the village level. So we, we have a hundred villages, so it's fine. So, I mean, it's not like we've been very, uh, conservative. I mean, you know, that's not, not for the average treatment effect, but for the, the, the heterogeneous treatment effects. We don't have heterogeneous treatment effects. I know. I know if you did, if you did what I was suggesting, write down a model and try to identify the heterogeneous treatment effects to understand the mechanism, you may be underpowered to estimate the heterogeneous treatment effects. Oh yeah. Yeah. Yeah. I understand. Okay. But you're mainly talking about power for the average treatment effect, which is fine. Yes. Yeah. Yeah. That's good. That's true. That's very true. Um, okay. So I've, I've explained this. You can also using, use the same. Once you, since you recalibrate these means to be representative of the control village population, you can also compare them to each other. So once you have recalibrated them, you should compare them. So you can compare one group of treated with, to another group of treated. They've, they've been, you know, um, I've already explained this. I mean, the easiest way to explain how you recover the properties, the easiest way to, to understand it is that if you think that each person is a different X vector, so then you, you would, uh, you know, run a bunch of simulations. I think we've, we've run 500 simulations. So you would run, um, let's say first simulation, um, DLIP gets assigned to be a teacher. And so therefore it's one, zero, zero, zero. So that's one sample. And then in another, some, in another simulation, he gets assigned to be a nominating student. So he's here and then he's there. Okay. Basically you do this for other times. Some of them get frequencies, frequency, how often does DLIP get to be a teacher? You know, maybe a lot of times. So therefore he's going to be, you know, he's overrepresented if you want, in the, in the teacher population. So we're going to want to, uh, wait, his, to reweight the teacher population so that he gets less weight. That's the, that's the idea. So that's how this is done here. All right. Now the thing you have to worry, if you do that, the thing you have to worry is that people who never get assigned to a particular, uh, role, because then, then, as I said, they, they, they basically, uh, don't have a kind of comparable person. So, so this is not what we get. So these are basically the, the, these, uh, these normalized, uh, probabilities that the weights that we are using and, um, the sample, and, and so, you know, they are first centered on one, which is the mean, and, uh, it doesn't even reach 0.5 and it doesn't go, it, you know, the couple observations above couple people above, uh, above two, but very few. So basically the, the range of values of these, of these weights is not crazy. It stays within a relatively narrow range. And so that means that, um, the reweighting we are doing is not, is not, is not insane. So it's not like one person is 99% of the time, always a teacher. And then you, you, you take his inverse sampling where it's going to be one over 99. And so there's, you know, it's going to be, so that is going to be very large. There's some no, it would be, it would be a 99 over one. So it would be very small. Okay. Anyway, so the, so that you, you want to look whether you have very large value, very small, obviously don't have that. So that's actually quite a, very encouraging balance. There are lots of tables about balance in the, in the paper. They are in the appendix, um, balance across the different types of treatment. And there, we, uh, we, we, we don't find any problem with balance. Note that we do balance, uh, controlling for sampling weights, and we also cluster standard errors by village. So, so we try to do this the same way we would perform the rest of the analysis. So this is the, this is the first table I have. These are just descriptives and well, a bit more than the script, sorry. I've saw some, I have some tests, but the first part of the table, if you want the first half of the table, the top half, is about, um, perhaps, uh, you know, issues of possible interest. One is, uh, you know, how do people score on the test? Whether the test has got, um, actually should be zero to one to eight. The test has eight questions, so they could either answer them all wrong. I think it should, should, should say zero to eight, or they could answer, I think nobody does that, but they could, they could answer all of them right. Then the average is about seven. So they're pretty good at remembering what the correct answer is. This is on, in general, not that we didn't ask these questions in the control villages, but we know from, uh, from other observational data that they don't know much about SRI. Um, and this is just another variable we use is whether they can, uh, answer the three main questions correctly and you get, you get 88% of farmers who can, treated farmers, in treat, not treated farmers, farmers in treated villages who can do that. In terms of nomination, remember, they could make up to, they could make up to five nominations. They actually make, make nearly all five of five nominations. So it's very stable. We don't have to worry too much about some of them nominating one because he's the only one who matters. And then some nominating five, like nearly everybody nominates five. So that, that helps us as well. And now we already have, uh, I can already show you some of the average, uh, treatment effects, uh, or at least the, the, the means, the, the, the, the means in each of the samples. So, because I, I have the means in the control villages. These are, these are the, the, the, the adoption variables that I have. Um, this is just basically comparing control villages to treated villages. These are means, village level means. Okay. And of course, I'm going to be looking at more than that, but so the first variable is basically whether BRAC estimate that, that the farmer has adopted at least some of the practices of SRI and in control villages by, you know, about two and a half percent of the farmers in that village are estimated by BRAC as, uh, so basically less than one farmer, um, while in treated villages about a third of the farmers. Okay. Okay. This is across the whole village. Um, in terms of the proportion of land, and of course that, that is massively significant. Uh, we have 3000, I forgot to tell you the, the sample size, but we, but you know, we have 30 farmers times a hundred villages where you have 3000 farmers altogether. So this kind of difference across, uh, 60 villages versus 40 villages, you know, you know, it's certainly significant. Um, proportion of land is just a percentage of the, of their cultivated acreage. Uh, that is, uh, that is on which SRI, uh, practices are followed. Um, so that's, uh, that, that's there again, you get, you see that, uh, in treated villages, you get a much bigger proportion. These are the number of SRI principles adopted on the plot in terms of villages, actually, they, they are recorded as, you know, adopting 1.4, but that could just be that they by chance, uh, adopted one of those, uh, one of those, uh, um, practices, but very few of them adopt more than one. So there's basically, it's, it's, it's a means, it looks like a high mean, but it's not the same practice that every farmer adopts, right? So they, they basically randomly adopt certain things, not knowing that this is called SRI. Um, very few of them, this is very important, very few of them actually follow the, the, the, the transplanting rule, which is about the age of the seedlings, very few in control villages, very few of them, um, follow the, the recommended distance between bundles, the, the places where they, they transplant the rice. Um, but it's, it's, it's actually quite a bit higher in, in, um, in treated villages, even though this still remains very low, but they still don't like to follow the age of the seedlings. So it's certainly not complete adoption. And then, um, and then in terms of number of seedlings per bundle, uh, you have a bit higher, uh, prac, you know, this practice is already kind of, uh, somewhat there, even in control villages, but even there is higher in, uh, treated villages. So that's, that's in terms of practices, what actually did the, so that you can see, we're not, it's not like you say, I, you know, have you adopted this fertilizer, meaning, have you uses fertilizer on any of your, of your fields here, you have, you know, uh, a whole bunch of, uh, uh, six main recommendations from SRI recommendation. And they basically, um, what is being tracked here are the main recommendations that they make. Uh, these are, this is the data that we have on, uh, agricultural performance at end line. We have yield value of crop output, input costs, labor costs, um, uh, total costs and profits. And here, these are all in, uh, so this is in kg per decimal. A decimal is a very small unit of, uh, uh, land, land, land area, uh, used in, uh, in Bangladesh. Um, and these are Bangladeshi taka per decimal. So these are in local currency and you can see they, they are differences, uh, like for instance, it's about 30, some taka difference here. Uh, here there's, there's very basically if anything, it's lower in terms of costs. Uh, it's the same in terms of labor costs on average and it's the same in terms of total costs. So there's a little bit of a higher value of output. And so you get a little bit of higher imputed profit. Okay. So that's the, and these, these things are significant. There's also something significant. This is, even that is significant, even though it's, it's reasonably small. So these are all average comparisons. So now we, we haven't divided, uh, uh, farmers in treated villages into the different, uh, treatment categories. So let's, let's, let's do that now. So here we divided them into teacher training, nominated, nominating student, non-dominating student, non-student. These are the control means that I just showed you a minute ago. And these are the adoption levels. So you can see immediately that, that, you know, the teacher trainings adopt a very high probability, uh, about 70% of them. The, the, the, these are the dependent variable is, is, is, uh, is, uh, is, it takes a value 100 if they adopt and zero if they don't adopt so that each coefficient is easily readable as a percentage point. Um, and remember the, the baseline level of adoption is very small. So, so everybody here is significantly above that, that, that, that value. So they all are quite significant. Once you, and then if you start looking at the, at the rest of the table, you get lots of, uh, lots of stars. So basically, you know, even here where even in control villages is, you know, close to 18% of the farmers who follow this practice, uh, anyway, but then you get a much higher, much higher, uh, these are, these are, you, you, you know, you have to add 18 to 17 to get, uh, how much a non-luminating student, uh, how much he follows the number of seedling per bundle. That's the way to read the table. So that's the intercept if you want. Are these about just the plot on which they're doing it or the plots on which they're doing it? Or is it about- So here, here, all the plots are included. So this is basically a regression that runs by plot. Uh, that's why there are some, there are more than 3000 observations because farmers could, uh, could mention up to three plots. They collected information that the three biggest plots, they collected information on that. And this basically what the focus of the study is, is because BRAC is not going to go and visit all your little tiny little thing. So they basically focus on up to three plots per farmer. And these tend to be the biggest ones and rice could typically be included. So that that's okay. So that's the, that's the way it's done. Um, so, so basically the, the lesson from this is that yeah, we've got pretty good effect on, on all these practices on adoption as measured, you know, in a relatively generous way here, um, compared to, uh, farmers in cultural religions. So that's the, that's the first lesson. And then if you look, well, you know, these numbers are different. So what about this number compared to this one? What about this number compared to this one? And so on. So what about this number? So what, so here we just basically using these regressions. So for the first column, we use this regression and we just do these pairwise theta. So, you know, is the coefficient, um, of teacher trainee significantly different from the coefficient of nominating students. So basically we do these pairwise comparisons to see whether there's a difference in treatment effects. And you can see that when we comparing teachers and students, you always get, these are p-values here. So we just report p-values, um, this is a test. And, uh, so they're basically, they are, they are more adopting than other people. You can see lots of zeros everywhere. There's zeros everywhere here. It's the teachers are clearly overwhelmingly adopting more, uh, SRI. In terms of the students, um, you can see that both, both students, you know, the, all of them is like also nearly all, uh, there's one, two here, but other than that, they're pretty high, uh, pretty low p-values. So again, a high, uh, significant effect that they, they are more likely to adopt than non-students in treated villages. Okay. So it's not just that there was some kind of diffusion in the whole village and they benefited from it. It's that they, basically the fact that they were targeted by our intervention through a teacher, uh, a trainee to receive SRI instruction, it actually made them more likely to adopt, uh, these SRI practices. Um, there is the only place where there is, except for this one, which is borderline significant, I would say everything else here is not significant. So basically this idea that we, we had, which is basically behind the model farmer, uh, logic, the idea that people would pay more attention if they are matched to somebody they look up to, somebody they regard as an opinion leader, as a, as a, as a source of valuable information. Um, and so if this was the case, you know, being matched with a model farmer, somebody you regard as a, as a model, uh, makes you more likely to adopt what they tell you to do, what they teach you, um, then you would expect this number, you would expect all these numbers to be positive and significant. They're all negative. Yeah. Thankfully they're not significantly negative, but they all negative except for this one, she's borderline significant. But, but Marcel, the, the teacher trainee has been nominated by somebody, right? They're all above median nominations. Isn't that? No, no, no, no, no, no, no, no, no, no. There's four above median and two below median. Okay. So is this coming from the above median or the below median, or is it the same no matter which? So what I'm trying to get at is I think your interpretation was it doesn't, you don't have to be a model farmer. You're equally effective at disseminating, I think is what you were saying. But, but I'm saying that out of six, four were considered, uh, modeled by, by a larger number of people. Right. So I may not have nominated them, but my neighbor nominated them. So they're probably still pretty good. Yeah. Yeah. It's a good idea. We could do that. We haven't done that, but we could do that. We could compare. We could do that. That's that's that goes in the direction of what Dilip was mentioning earlier, basically looking at the, yes, we could also, I mean, we haven't done that. We, we, of course we have the, the whole network of, uh, recommendations. So we could see whether the extent, you know, how much variation we have in centrality for, in degree centrality, in this case in centrality, or even a more sophisticated measure of centrality. Yeah. This is, this is something we could do. Yeah. Yeah. This is a good suggestion. Thanks. Thank you. Um, okay. But in terms of, uh, being matched to somebody you didn't nominate doesn't seem to disenfantage you here. So, um, but I see what you hear, what you're saying. And you say, well, maybe it's because, you know, what if you have been matched to somebody who was nominated by lots of people, maybe that that's, that's what the role model would be. Okay. Okay. So I, for that, I need to document the fact that what is the variation in centrality and, uh, because if there's not much variation in centrality, everybody mentions their five neighbors and it basically everybody's the same. There's no, there's no central point. That is a good, yeah. It's very good point. I will do that. I will do that. If you will add to the paper. Thanks. Um, okay. So this is now again, looking at the average treat, no, I have to be careful because I only have 15 minutes left. Um, the average treatment on, um, on agricultural performance. These are now in log. Because, you know, there's noise in the data, especially these values is always noise. So if you take log, it kind of dampens a little bit, uh, the upper tail and you don't have the winds or ice and all that. So, so the, the, the news is that in terms of yields, you get, you get slightly higher yields. This is so basically 7% higher yield for teacher trainee, 4% for the 5% for the students. And in terms of profit, they get about four, you know, the teachers, uh, get about 14% higher profits, 9% and 14%, um, uh, compared to the, to the control. So, so there is a, there is some benefit that, that is, uh, that is, you know, achieved through that. It's not just that they learn to, to, uh, to mimic what had been told. Uh, they also actually benefit, uh, from that. So that, that's the good news. Um, so now we, we, so that's, that's all about average treatment effects. So now I want to focus a little bit more about, uh, I want to do a little bit more. One is the, I want to compare, um, uh, the students, uh, um, um, depending on whether their teacher is incentivized or not. So basically the, this is the mean for unincentivized, uh, teachers, and this is the, the mean, the, the additional increase in score, um, in, in the quiz of, of students who have been assigned, uh, to an incentivized, uh, teacher training. And you can see there's a little bit significant or, you know, mildly significant, uh, improvement in, in, in, in, it's a little bit stronger if you look as to whether they answer the three main questions correctly. Um, so that means that incentivizing the teacher seems to have induced the teacher to do a slightly better job in teaching them, uh, to the test, of course, in teaching them to take the test. Okay. Um, and this is whether, we also look as to whether it has an effect on the teachers. So basically do the teachers do better on the test because they, they, they taught, uh, they had to, they knew they had to teach the test. There we don't see, well, there's a positive, but, you know, coefficient here, but it's definitely not significant. And you can see the, the teachers do in general very, very well on the test. Okay. So it looks like they were paying attention, uh, knowing that they're going to be teachers, not, doesn't really depend on them being incentivized. Um, this is, um, um, this is the effect of, or rather the absence of effect of, so you see that if you incentivize the teacher, the students do a little bit better on the test. Does it translate in higher adoption? The answer is no. So the, the, yeah, the, we have slightly, you know, the, the point estimates are very small. Uh, the, nothing is ever significant. There is a very large relative to the coefficient. So basically there is no, but incentivization did not induce more adoption. Okay. And it also did not induce more adoption by the teachers. The reason why we worry about that is, is, uh, because in the other paper, the one we did, on referrals, what we found is that the farmers have referred or the farmers, um, the other farmers kind of learn about that. They know that they've been referred. Oh, yeah, Marcel, you referred me. Okay. And then, so Marcel suddenly decides, oh, me now he's observing me. He's, he, you know, he knows that I referred him. So maybe I should put my money where my mouth is. So basically, you know, I better adopt a little bit, or make you pretend to adopt a little bit so that I don't completely, don't look completely silly to have recommended this person for training. Otherwise you would say, why did you send me something completely useful to, to do something completely useless, pardon? Sorry. So, so that's why we, we, we, just looked this to whether they didn't do that here. They don't know that we did not induce excess adoption by teachers by incentivizing them, which is, I think is good news. Um, I think that's a good news. So, so this is about incentivization. This is about the other treatment, whether you are assigned to a teacher that you nominated or not. So does it affect the performance of the student, uh, on the quiz? Well, there's a, there's a point estimate, which is positive here, but it's definitely not significant. Same thing here. This is very small. So basically the answer is no. The fact that you are assigned to somebody you, you regard as you listed as a role model doesn't affect how much you learn from them, even on the quiz. Um, and this is what happens, um, on your adoption itself. If anything, there's one, one thing here that's negative, but, uh, which I kind of already saw before borderline significant 10% level. But other than that, it's all, it's all negative. And so, yeah, let's just confirm what we already looked at before. What about mediation analysis? It's going into a bigger direction of, um, heterogeneity analysis. So, so here basically we're gonna, we're gonna, we're gonna think about what could be the process by which the teacher is, um, is inducing adoption. Is it because he's transferring more knowledge and this knowledge is useful for adoption? Okay. Well, you have to know what the practices are before you can actually follow them and be measured as following them. Or is it that people follow the example? So basically, you know, if the, if the teacher is adopting and maybe you can, you can take the, the trainees to the field or at least you can talk them through it on the field. So that, so that's the, so to do that basically, we thought, well, why, why don't we put, um, that should not be, uh, so this is the incentivize teacher trainee. And then we thought, well, if it is, uh, uh, no, this is, this is not, no, this is the quiz performance of the, what happens here? Sorry. I think this is not the teacher's value. It's the, it's the student's value of the, so basically if, if the effect of, um, it is, no, no, this is, okay. Sorry. Sorry. I got confused for a second. This is the effect on the, on the, on the, the, the, the, the, the students' performance in the quiz. This is the student and the quiz and basically students whose teacher does better on the quiz, um, also do better on the quiz. So basically there's some transmission of, of knowledge from the teacher to the student. So state teachers who learn more, transit more to their, to their students, even controlling for the fact that the teacher may have been incentivized. So we control for that. So that here's a control. We're just looking whether the transmission is through, or at least some of the transmission is through knowledge transmission. The teachers who learn more transfer more to their, to their, um, uh, to their students. And this is by nominate the same thing by nomination status. So again, you know, it is also there. Um, this is the effect of the, of the quiz score on, on, uh, adoption. And so basically you look as to whether this has an effect on adoption as well. And so you get, you get a couple significant terms there, but other than that, it seems to be pretty flat. And the same thing, uh, is reproduced if you, if you compare nominating students, non nominating students, instead of comparing incentivized, uh, teacher training students to, uh, non-incentivized teacher training students. So, so basically what this says is that, uh, performance on the quiz score, uh, of, of the teacher just, just has some, some transmission to, uh, uh, to, to adoption suggesting that knowledge is, is one of the channels of transmission as you would expect. I mean, you would, you would hope this is true, but, but now we have to look at the other thing, which is, um, um, uh, examples teaching by example. So, so here we just, we're looking at the teacher's value of the, the corresponding adoption measure. So basically, are people more likely to adopt is if the teacher adopts, are the people more likely to plant on a large proportion of their land, if the teacher does that and so on. So basically, we, we can see that in all these cases that these things are, are, are positive. The only exception being this one where teachers are already quite high to start with. So, so there's not, there's perhaps a bit less variation there, but so what we find is that it's basically the evidence that both channels are operating here. One is that if you are taught by a better teacher, someone who has paid more attention to the course, knows more, you do, you adopt more. It's a channel of transmission and you also better, you also know better on the quiz. And, um, and then the other thing is that, uh, if the teacher adopts, you, you seem to, you're more likely to follow his example. So these two things are there. So the last thing I'm going to do, um, in the three minutes I have, I should have stopped earlier, but, um, um, so we also would like to know whether inviting trainees to be a teacher has an effect of, of, on their adoption. Okay. Not, not relative to their students, but relative to trainees who would not have received this task of training of the students. So fortunately we have this other, uh, experiment we ran, you know, it's also an RCT. Um, and in that experiment we have a whole batch of, um, trainees who were randomly selected among the 30 farmers and were only trained. They only received the training. So we can kind of compare them to the, to the teachers. So that's basically what we do in this, in this first comparison here. And what you find is that, so P2P the me is basically means that you were in the, in the teacher training experiment and not in the other experiment. And what you find is that, so if, if the average adoption in, uh, of the, of the, uh, of the randomly selected trainees in the other experiment was 36%, then among, uh, our teacher trainee, you know, we know it's, it's like close to 70%. So it's basically 34% higher than that. Okay. So that's the, that's the idea. So basically what you get is that, um, inviting people to be teacher training makes them adopt more. Okay. Um, and it doesn't necessarily do that for the, for the, for the, you know, if you, these, these, these other comparisons are not more significant, but perhaps the surprise is that, um, the, the student farmers, if you compare them to, uh, to, to, to, to other trainees, they not that, you know, sometimes a little bit less, sometimes a bit more, but they're not massively different from people who actually received the BRAC training, which I think is quite remarkable. These are, these are people who have not seen any BRAC, uh, uh, training, uh, agent, but they basically all they have learned about SRIs from other farmers and, and their adoption and behavior in more general is relatively similar to, to people who in the other, um, in the other, uh, experiment receive a training directly from BRAC. We do a small, um, uh, back of the envelope cost benefit analysis. The details of, uh, of the calculations are in the appendix. We get a, uh, we estimate a social return of two point, basically you value the profits and so on, and compare it to the cost that we have incurred. And you, you know, you get a two point, two and a half dollars per dollar invested in the program, which generates a rate of return, social rate of return 150%, which is not bad. It's lower than what we got in the other experiment, but the other experiment, suggested that, um, um, if you incentivize me, if you push people, they tend to over adopt. So they basically, you kind of induce them to experiment with, uh, with the technology and then everything experimented, they discover it's not for them. It's too difficult for them. And then they withdraw from the technology the year after we noticed that. So here by inducing more people to adopt with, you know, it's great. We have induced lots of people to experiment, but as, as we said, not SRI is not for everybody. And so, so as a result, maybe there's excess, you want excess experimentation. I don't know if it's excess, but you get lots of people to experiment who don't necessarily benefit from it. Certainly not in the first year they try it. So that's the, that's the kind of the general, uh, lesson here. And, uh, maybe I should, I should stop here. And, uh, if there's still time for, for questions, if people still have questions. Do you have any questions from the audience? So Marcel, just to follow up on Dilip's earlier question, uh, you don't really look at heterogeneity with respect to any characteristics of either the teachers or the students. No. Yes. And is, uh, and that because of lack of power or lack of data or, or lack of you, or you think it's not really identified. It's probably due to perceive lack of patience for it by, uh, editors and referees. I think that's the, oh, you are picking winners. You go, you have 27 heterogeneity tests. You just report the one that gives you a big kick and so, and then in any case, it's not, uh, it's not as, it's not experimentally assigned and so on. So we, we try to, but I think that the, but you have given me some suggestion for, for, uh, I really liked the suggestion earlier about looking at, uh, nominations. I think it's a very good suggestion and we should do that in the paper because it really fits in the paper. It fits in the motivation of the paper and, uh, it's, it's supposed to be related to what Dillipo saying as well. So, I had, I had another sort of broader question here. It's, I mean, can you reject the, uh, story where there's this one kind of heterogeneity, which is a skill and the nominations basically were a way to identify the more skilled farmers. Uh, and the students themselves were selected by BRAC on the basis of something which I probably assume is related to farming skill. So you've already got a preselected, you know, high skill farmers and then within them you're selecting the most skilled and they just happen to be teachers. So can you reject the hypothesis that it's just about selection of skill that all this teaching stuff is, you know, it's not doing anything. Uh, I mean, of course, yeah, there's some diffusion obviously, uh, but you know, but basically, you know, it's not nothing to do with skill in teaching or anything like that. I mean, it's just skill in picking up stuff that's now generally available in the village. No, but I mean, that's fine, but No, but it's not true that we've only selected as teachers, those that were the most skilled as measured by the number of nomination they receive. We have four, four people among the, You know, but it was biased in favor of the people with high, high nominations. So I think, you know, doing what Sujata was suggesting is a good idea is, you know, actually why not just look at the number of nominations, uh, that you received and just look at, you know, how performance varied with that. That, that would be one way of getting at it. Yeah. Okay. Yeah. Hey, yeah, I can do that. I can do that. But remember in terms of the test itself, I mean, we have corrected for the stratification. So, so I don't think that's the, that's, that's, I don't think that is invalidating our results because we have, we've thought about it. No, no, it's a different interpretation. I'm just saying, I'm not disputing the statistical validity of your results. It's just a question of what, what you think is sort of going on underneath and whether teaching has anything to do with it. Yes. So in the first experiment, the one with the 182, whatever villages where you didn't ask them to teach, but did you select them the same way? It was simpler because we were not trying to match, but okay. So the, the, the, the, the, the guy can, so in that experiment, the, the start is the same. We take 30, Brack shows up, picks their 30 farmers, because that's the way they, they kind of line up farmers for SRI training. So they kind of identify them and then they keep them and they, then they, they bring the, the training agent. So, so, and they train them in groups of 12. So basically we basically randomly selected 12 farmers of the 30 and assigned them to, from the first training session. And then at the end of the training session, each of them was asked to, so there were 18 farmers left on the list. So before they left, they say, oh, before you leave and because they get paid their little stipend. So before you, you know, here's a list, pick a name on the list as to who would most benefit from the, the, the BRAC training. So they basically look at the list and say, oh, okay. As I can see, Albert's in the list. So I'm going to mention Albert. And then the next person comes and say, oh, I'm going to mention Albert too. No, no, no. Look, it's already been crossed. You can't mention Albert anymore. You have to mention someone else. Okay. I'm going to mention Chujata. So they basically mentioned someone else. Basically then you can see the people who have been, but in that paper, we, we actually use that, that order, if you want to see, the bottom line is that they did not, they did not recommend on average. So remember, they recommend 12 people out of 18. So normally you should, you should, you know, that the six worst farmers, the least able farmers, the least skilled farmers should have been dropped out of the list. So they, you should basically see that this selected sample of 12 out of 18 farmers should be better than the 12 randomly selected farmers. Right. Because the second sample is that is only two thirds of the 18 and they've been selected to be the more skilled farmers. So you should see that they are adopting more because they understand what's going on and they, they get more, more benefit from it. That's not that what we find at all. So the reason I was asking was, I was wondering if that helps you answer Dilip's question about whether there's something about having to teach that makes you a better adopter yourself, but you can't actually answer that question because the way that these teachers, those guys were selected is different. It's random. They're not both being selected the same way and then some get to teach and others don't get to teach. It's not as simple as that. But I mean, everybody could have been selected to teach. Everybody has a probability of being selected. If you are over 15 above medium, you have four out of 15 chances. If you are over 15, you have two out of 15 chances. Everybody could be a teacher. And we correct for the probabilities of you being a teacher. Yeah. Yeah, that I do understand that. Okay. I think we're a little bit past time. So I'd like to thank myself. Very interesting presentation. And I'd like to thank, I also want to thank the staff at IEMS who have been coordinating the webinars for all six seminars. So that's Carla, who's managing today, Carla Chan and Joey Chu, who's the manager at IEMS. I think it's been really well managed. So, you know, silent applause. And this was obviously a second best event in lieu of having a workshop in person. But I think it's been really valuable in the world of COVID to still be able to have some intellectual exchange and bring, you know, leading people working on common issues of interest. So I want to thank all the participants. Yeah, I appreciate very much you having me. I really enjoyed it. And thanks for the excellent comments. Thank you. Thank you. Thank you, Marcel. Goodbye, everyone. Thanks. Bye. Bye. Bye. Bye.